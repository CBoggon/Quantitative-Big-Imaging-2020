{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ETHZ: 227-0966-00L\n",
    "# Quantitative Big Imaging\n",
    "# April 11, 2019\n",
    "\n",
    "## Dynamic Experiments: Feature Points\n",
    "\n",
    "#### Anders Kaestner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams['font.family'] = ['sans-serif']\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Papers / Sites\n",
    "\n",
    "- Keypoint and Corner Detection\n",
    " - Distinctive Image Features from Scale-Invariant Keypoints - https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\n",
    " - https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Key Points (or feature points)\n",
    "\n",
    "- Registration using the full data set is time demaning.\n",
    "- We can detect feature points in an image and use them to make a registration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Identifying key points\n",
    "We first focus on the detection of points. \n",
    "\n",
    "A [Harris corner detector](https://en.wikipedia.org/wiki/Harris_Corner_Detector) helps us here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.feature import corner_peaks, corner_harris, BRIEF\n",
    "from skimage.transform import warp, AffineTransform\n",
    "from skimage import data\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "tform = AffineTransform(scale=(1.3, 1.1), rotation=0, shear=0.1,\n",
    "                        translation=(0, 0))\n",
    "image = warp(data.checkerboard(), tform.inverse, output_shape=(200, 200))\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax1.imshow(image); ax1.set_title('Raw Image')\n",
    "ax2.imshow(corner_harris(image)); ax2.set_title('Corner Features')\n",
    "peak_coords = corner_peaks(corner_harris(image))\n",
    "ax3.imshow(image); ax3.set_title('Raw Image')\n",
    "ax3.plot(peak_coords[:, 1], peak_coords[:, 0], 'rs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's try the corner detection on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "full_img = imread(\"ext-figures/bonegfiltslice.png\")\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax1.imshow(full_img)\n",
    "ax1.set_title('Raw Image')\n",
    "ax2.imshow(corner_harris(full_img)), ax2.set_title('Corner Features')\n",
    "peak_coords = corner_peaks(corner_harris(full_img))\n",
    "\n",
    "ax3.imshow(full_img), ax3.set_title('Raw Image')\n",
    "ax3.plot(peak_coords[:, 1], peak_coords[:, 0], 'rs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Tracking with Points\n",
    "\n",
    "__Goal:__ To reducing the tracking efforts\n",
    "\n",
    "We can use the corner points to track features between multiple frames. \n",
    "\n",
    "In this sample, we see that they are \n",
    "- quite stable \n",
    "- and fixed \n",
    "    \n",
    "on the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We need data - a series transformed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "fig, c_ax = plt.subplots(1, 1, figsize=(5, 5), dpi=100)\n",
    "\n",
    "\n",
    "def update_frame(i):\n",
    "    c_ax.cla()\n",
    "    tform = AffineTransform(scale=(1.3+i/20, 1.1-i/20), rotation=-i/10, shear=i/20,\n",
    "                            translation=(0, 0))\n",
    "    image = warp(data.checkerboard(), tform.inverse, output_shape=(200, 200))\n",
    "    c_ax.imshow(image)\n",
    "    peak_coords = corner_peaks(corner_harris(image))\n",
    "    c_ax.plot(peak_coords[:, 1], peak_coords[:, 0], 'rs')\n",
    "\n",
    "\n",
    "# write animation frames\n",
    "anim_code = FuncAnimation(fig,\n",
    "                          update_frame,\n",
    "                          frames=np.linspace(0, 5, 10),\n",
    "                          interval=1000,\n",
    "                          repeat_delay=2000).to_html5_video()\n",
    "plt.close('all')\n",
    "HTML(anim_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Features and Descriptors\n",
    "We can move beyond just key points to keypoints and feature vectors (called descriptors) at those points. \n",
    "\n",
    "A descriptor is a vector that describes a given keypoint uniquely. \n",
    "\n",
    "This will be demonstrated using two methods in the following notebook cells...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.feature import ORB\n",
    "full_img = imread(\"ext-figures/bonegfiltslice.png\")\n",
    "orb_det = ORB(n_keypoints=10)\n",
    "det_obj = orb_det.detect_and_extract(full_img)\n",
    "fig, (ax3, ax4, ax5) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax3.imshow(full_img, cmap='gray')\n",
    "ax3.set_title('Raw Image')\n",
    "for i in range(orb_det.keypoints.shape[0]):\n",
    "    ax3.plot(orb_det.keypoints[i, 1], orb_det.keypoints[i,\n",
    "                                                        0], 's', label='Keypoint {}'.format(i))\n",
    "    ax4.bar(np.arange(10)+i/10.0, orb_det.descriptors[i][:10]+1e-2, width=1/10.0,\n",
    "            alpha=0.5, label='Keypoint {}'.format(i))\n",
    "ax5.imshow(np.stack([x[:20] for x in orb_det.descriptors], 0))\n",
    "ax5.set_title('Descriptor')\n",
    "ax3.legend(facecolor='white', framealpha=0.5)\n",
    "ax4.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Defining a supporting function to show the matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.feature import match_descriptors, plot_matches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_matches(img1, img2, feat1, feat2):\n",
    "    matches12 = match_descriptors(\n",
    "        feat1['descriptors'], feat2['descriptors'], cross_check=True)\n",
    "    \n",
    "    fig, (ax3, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    c_matches = match_descriptors(feat1['descriptors'],\n",
    "                                  feat2['descriptors'], cross_check=True)\n",
    "\n",
    "    plot_matches(ax3,\n",
    "                 img1, img2,\n",
    "                 feat1['keypoints'], feat1['keypoints'],\n",
    "                 matches12)\n",
    "\n",
    "    ax2.plot(feat1['keypoints'][:, 1],\n",
    "             feat1['keypoints'][:, 0],\n",
    "             '.',\n",
    "             label='Before')\n",
    "\n",
    "    ax2.plot(feat2['keypoints'][:, 1],\n",
    "             feat2['keypoints'][:, 0],\n",
    "             '.', label='After')\n",
    "\n",
    "    for i, (c_idx, n_idx) in enumerate(c_matches):\n",
    "        x_vec = [feat1['keypoints'][c_idx, 0], feat2['keypoints'][n_idx, 0]]\n",
    "        y_vec = [feat1['keypoints'][c_idx, 1], feat2['keypoints'][n_idx, 1]]\n",
    "        dist = np.sqrt(np.square(np.diff(x_vec))+np.square(np.diff(y_vec)))\n",
    "        alpha = np.clip(50/dist, 0, 1)\n",
    "\n",
    "        ax2.plot(\n",
    "            y_vec,\n",
    "            x_vec,\n",
    "            'k-',\n",
    "            alpha=alpha,\n",
    "            label='Match' if i == 0 else ''\n",
    "        )\n",
    "\n",
    "    ax2.legend()\n",
    "\n",
    "    ax3.set_title(r'{} $\\rightarrow$ {}'.format('Before', 'After'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's create some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.filters import median\n",
    "full_img = imread(\"ext-figures/bonegfiltslice.png\")\n",
    "full_shift_img = median(\n",
    "    np.roll(np.roll(full_img, -15, axis=0), 15, axis=1), np.ones((1, 3)))\n",
    "\n",
    "bw_img = full_img\n",
    "shift_img = full_shift_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Features found by the BRIEF descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.feature import corner_peaks, corner_harris, BRIEF\n",
    "\n",
    "\n",
    "def calc_corners(*imgs):\n",
    "    b = BRIEF()\n",
    "    for c_img in imgs:\n",
    "        corner_img = corner_harris(c_img)\n",
    "        coords = corner_peaks(corner_img, min_distance=5)\n",
    "        b.extract(c_img, coords)\n",
    "        yield {'keypoints': coords,\n",
    "               'descriptors': b.descriptors}\n",
    "\n",
    "\n",
    "feat1, feat2 = calc_corners(bw_img, shift_img)\n",
    "show_matches(bw_img, shift_img, feat1, feat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Features found by the ORB descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.feature import ORB, BRIEF, CENSURE\n",
    "\n",
    "\n",
    "def calc_orb(*imgs):\n",
    "    descriptor_extractor = ORB(n_keypoints=100)\n",
    "    for c_img in imgs:\n",
    "        descriptor_extractor.detect_and_extract(c_img)\n",
    "        yield {'keypoints': descriptor_extractor.keypoints,\n",
    "               'descriptors': descriptor_extractor.descriptors}\n",
    "\n",
    "\n",
    "feat1, feat2 = calc_orb(bw_img, shift_img)\n",
    "show_matches(bw_img, shift_img, feat1, feat2)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "footer": "April 9, 2020 - ETH 227-0966-00L: Quantitative Big Imaging/Dynamic experiments: Feature points",
   "header": "<table width='100%' style='margin: 0px;'><tr><td align='left'><img src='../common/figures/eth_logo_kurz_pos.svg' style='height:30px;'></td><td align='right'><img src='../common/figures/PSI-Logo.svg' style='height:50px;'></td></tr></table>",
   "scroll": true
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
