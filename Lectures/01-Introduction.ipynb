{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# ETHZ: 227-0966-00L\n",
    "# Quantitative Big Imaging\n",
    "# February 20, 2020\n",
    "\n",
    "## Introduction and Overview\n",
    "#### Anders Kaestner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "- Who are we?\n",
    "- Who are you?\n",
    " - What is expected?\n",
    "- __Why does this class exist?__\n",
    " - Collection\n",
    " - Changing computing (Parallel / Cloud)\n",
    " - Course outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "- What is an image?\n",
    "- Where do images come from?\n",
    "- Science and Reproducibility\n",
    "- Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anders Kaestner, PhD (anders.kaestner@psi.ch)\n",
    " - __Beamline scientist__ at the ICON Beamline at the SINQ (Neutron Source) at Paul Scherrer Institute\n",
    "     - __Lecturer__ at ETH Zurich\n",
    " - __Algorithm developer__ Varian Medical Systems, Baden-Daettwil\n",
    " - __Post Doc__ at ETH Zurich, Inst for Terrestial Ecology\n",
    " - __PhD Student__ at Chalmers Institute of Technology, Sweden, Signal processing\n",
    "<img src=\"../common/figures/anders.jpeg\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stefano van Gogh  (stefano.van-gogh@psi.ch)\n",
    " - Exercise assistance\n",
    " - __PhD Student__ in the X-Ray Microscopy Group at ETH Zurich and Swiss Light Source at Paul Scherrer Institute\n",
    " \n",
    " <img src=\"https://www.psi.ch/sites/default/files/styles/primer_teaser_square_scale/public/2019-06/picture.jpg?itok=t9wRh5Yb\" style=\"height:250px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Guest Lecturers (to be confirmed)\n",
    "\n",
    "## Kevin Mader\n",
    " - Data scientis at Apple\n",
    " - CTO at __4Quant__ for Big Image Analytics (ETH Spin-off)\n",
    " - __Lecturer__ at ETH Zurich\n",
    " - Formerly __Postdoc__ in the X-Ray Microscopy Group at ETH Zurich (2013-2015)\n",
    " - PhD Student at Swiss Light Source at Paul Scherrer Institute (2008-2012)\n",
    " \n",
    " <img src=\"https://avatars0.githubusercontent.com/u/116120?s=460&v=4\" style=\"height:250px\">\n",
    " \n",
    " Kevin developed this course and a great part of the slide are his work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Michael Prummer, PhD (prummer@nexus.ethz.ch)\n",
    "\n",
    "- Biostatistician at NEXUS Personalized Health Technol.\n",
    "- Previously Senior Scientist at F. Hoffmann-La Roche Ltd., Basel, Switzerland.\n",
    " - Pharma Research & Early Development (pRED), Discovery Technologies\n",
    " - Phenotypic Drug Discovery & Target Identification.\n",
    " - Topic: High Content Screening (HCS), Image analysis, Biostatistics, Image Management System.\n",
    " \n",
    "<img src=\"https://www.nexus.ethz.ch/people/person-detail.person_image.jpeg?persid=OTAxMDQ=\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Who are you?\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "    \n",
    "### <center>A wide spectrum of backgrounds</center>\n",
    "</td>\n",
    "<td>\n",
    "    \n",
    "### <center>A wide range of skills</center>\n",
    "</td></tr>\n",
    "<tr><td>  \n",
    "            \n",
    "- Biomedical Engineers, \n",
    "- Physicists, \n",
    "- Chemists, \n",
    "- Art History Researchers, \n",
    "- Mechanical Engineers, \n",
    "- and Computer Scientists\n",
    "    \n",
    "</td>\n",
    "<td>\n",
    "    \n",
    "- I think Ive heard of python before \n",
    "- I write template C++ code and hand optimize it afterwards\n",
    "    \n",
    "</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# So how will this ever work?\n",
    "\n",
    "## Adaptive assignments\n",
    "\n",
    "### Conceptual, graphical assignments with practical examples\n",
    "  - Emphasis on chosing correct steps and understanding workflow\n",
    "\n",
    "### Opportunities to create custom implementations, plugins, and perform more complicated analysis on larger datasets if interested\n",
    "  - Emphasis on performance, customizing analysis, and scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Course Expectations\n",
    "<table>\n",
    "<tr><td>\n",
    "        \n",
    "## <center>Exercises</center>\n",
    "               \n",
    "</td>    \n",
    "<td>\n",
    "    \n",
    "## <center>Science Project</center>\n",
    "    \n",
    "</td></tr>\n",
    "    \n",
    "<tr>\n",
    "<td>\n",
    "    \n",
    "- Usually 1 set per lecture\n",
    "- Optional (but recommended!)\n",
    "- Easy - using GUIs (KNIME and ImageJ) and completing Matlab Scripts (just lecture 2)\n",
    "- Advanced - Writing Python, Java, Scala, ...\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "    \n",
    "- Optional (but strongly recommended)\n",
    "- Applying Techniques to answer scientific question!\n",
    " - Ideally use on a topic relevant for your current project, thesis, or personal activities\n",
    " - or choose from one of ours (will be online, soon)\n",
    "- Present approach, analysis, and results\n",
    "   \n",
    "    </td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Literature / Useful References\n",
    "\n",
    "\n",
    "## General Material\n",
    "- Jean Claude, Morphometry with R\n",
    " - [Online](http://link.springer.com/book/10.1007%2F978-0-387-77789-4) through ETHZ\n",
    " - [Buy it](http://www.amazon.com/Morphometrics-R-Use-Julien-Claude/dp/038777789X)\n",
    "- John C. Russ, “The Image Processing Handbook”,(Boca Raton, CRC Press)\n",
    " - Available [online](http://dx.doi.org/10.1201/9780203881095) within domain ethz.ch (or proxy.ethz.ch / public VPN)\n",
    "- J. Weickert, Visualization and Processing of Tensor Fields\n",
    " - [Online](https://doi.org/10.1007/978-3-540-88378-4) through ETHZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Today's Material\n",
    "\n",
    "\n",
    "- Imaging\n",
    " - [ImageJ and SciJava](http://www.slideshare.net/CurtisRueden/imagej-and-the-scijava-software-stack)\n",
    "- Cloud Computing\n",
    " - [The Case for Energy-Proportional Computing](http://www-inst.eecs.berkeley.edu/~cs61c/sp14/) _ Luiz André Barroso, Urs Hölzle, IEEE Computer, December 2007_\n",
    " - [Concurrency](http://www.gotw.ca/publications/concurrency-ddj.htm)\n",
    "- Reproducibility\n",
    " - [Trouble at the lab](http://www.economist.com/news/briefing/21588057-scientists-think-science-self-correcting-alarming-degree-it-not-trouble) _Scientists like to think of science as self-correcting. To an alarming degree, it is not_\n",
    " - [Why is reproducible research important?](http://simplystatistics.org/2014/06/06/the-real-reason-reproducible-research-is-important/) _The Real Reason Reproducible Research is Important_\n",
    " - [Science Code Manifesto](http://software-carpentry.org/blog/2011/10/the-science-code-manifestos-five-cs.html)\n",
    " - [Reproducible Research Class](https://www.coursera.org/course/repdata) @ Johns Hopkins University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation\n",
    "\n",
    "![Crazy Workflow](../common/figures/crazyworkflow.png)\n",
    "- To understand what, why and how from the moment an image is produced until it is finished (published, used in a report, …)\n",
    "- To learn how to go from one analysis on one image to 10, 100, or 1000 images (without working 10, 100, or 1000X harder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## High acquisition rates\n",
    "- Detectors are getting bigger and faster constantly\n",
    "- Todays detectors are really fast\n",
    " - 2560 x 2160 images @ 1500+ times a second = 8GB/s\n",
    "- Matlab / Avizo / Python / … are saturated after 60 seconds\n",
    "- A single camera\n",
    " - [More information per day than Facebook](http://news.cnet.com/8301-1023_3-57498531-93/facebook-processes-more-than-500-tb-of-data-daily/)\n",
    " - [Three times as many images per second as Instagram](http://techcrunch.com/2013/01/17/instagram-reports-90m-monthly-active-users-40m-photos-per-day-and-8500-likes-per-second/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Different sources of images\n",
    "### X-Ray\n",
    " - SRXTM images at (>1000fps) → 8GB/s\n",
    " - cSAXS diffraction patterns at 30GB/s\n",
    " - Nanoscopium Beamline, 10TB/day, 10-500GB file sizes\n",
    "\n",
    "### Optical\n",
    " - Light-sheet microscopy (see talk of Jeremy Freeman) produces images → 500MB/s\n",
    " - High-speed confocal images at (>200fps) → 78Mb/s\n",
    "\n",
    "### Personal\n",
    " - GoPro 4 Black - 60MB/s (3840 x 2160 x 30fps) for $600\n",
    " - [fps1000](https://www.kickstarter.com/projects/1623255426/fps1000-the-low-cost-high-frame-rate-camera) - 400MB/s (640 x 480 x 840 fps) for $400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "\n",
    "1. __Experimental Design__ finding the right technique, picking the right dyes and samples has stayed relatively consistent, better techniques lead to more demanding scientists.\n",
    "\n",
    "2. __Management__ storing, backing up, setting up databases, these processes have become easier and more automated as data magnitudes have increased\n",
    "\n",
    "3. __Measurements__ the actual acquisition speed of the data has increased wildly due to better detectors, parallel measurement, and new higher intensity sources\n",
    "\n",
    "4. __Post Processing__ this portion has is the most time-consuming and difficult and has seen minimal improvements over the last years\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Rmd_chunk_options": "time-figure, fig.width=12, fig.height=8",
    "autoscroll": false,
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How is time used during the experiment life cycle?\n",
    "<img src=\"https://4quant.com/images/slides/qmia/qmia-014.png\" style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## So... how much is a TB, really?\n",
    "\n",
    "\n",
    "If __you__ looked at one 1000 x 1000 sized image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.matshow(np.random.uniform(size = (1000, 1000)), \n",
    "           cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "every second, it would take you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# assuming 16 bit images and a 'metric' terabyte\n",
    "time_per_tb=1e12/(1000*1000*16/8) / (60*60)\n",
    "print(\"%04.1f hours to view a terabyte\" % (time_per_tb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overwhelmed scientist\n",
    "\n",
    "<table>\n",
    "    <tr><td>\n",
    "        \n",
    "- Count how many cells are in the bone slice\n",
    "        \n",
    "- Ignore the ones that are ‘too big’ or shaped ‘strangely’\n",
    "        \n",
    "- Are there more on the right side or left side?\n",
    "        \n",
    "- Are the ones on the right or left bigger, top or bottom?\n",
    "\n",
    "</td><td>\n",
    "\n",
    "![cells in bone tissue](../common/figures/bone-cells.png)\n",
    "        \n",
    "</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## More overwhelmed\n",
    "<table>\n",
    "    <tr><td>\n",
    "        \n",
    "### Many samples are needed\n",
    "- Do it all over again for 96 more samples\n",
    "- this time in 3D with 2000 slices instead of just one!\n",
    "</td><td>\n",
    "\n",
    "![more samples](../common/figures/96-samples.png)\n",
    "        </td></tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bring on the pain\n",
    "\n",
    "<table>\n",
    "<tr><td>\n",
    "    \n",
    "### Great variations in the population   \n",
    "- Now again with 1090 samples!\n",
    "- How to measure?\n",
    "- How to analyze?\n",
    "\n",
    "</td><td>\n",
    "    \n",
    "![even more samples](../common/figures/1090-samples.png)\n",
    "    \n",
    "   </td></tr> \n",
    "</table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## It gets better\n",
    "\n",
    "\n",
    "- Those metrics were quantitative and could be easily visually extracted from the images\n",
    "- What happens if you have _softer_ metrics\n",
    "\n",
    "<center>\n",
    "    \n",
    "![alignment](../common/figures/alignment-figure.png)\n",
    "\n",
    "</center>\n",
    "\n",
    "- How aligned are these cells?\n",
    "- Is the group on the left more or less aligned than the right?\n",
    "- errr?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dynamic Information\n",
    "\n",
    "- How many bubbles are here?\n",
    "- How fast are they moving?\n",
    "- Do they all move the same speed?\n",
    "- Do bigger bubbles move faster?\n",
    "- Do bubbles near the edge move slower?\n",
    "- Are they rearranging?\n",
    "\n",
    "<video controls loop src=\"../common/movies/dk31-plat.avi\" type=\"video/avi\" height=\"350px\"></video>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Course Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='hide'",
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json, pandas as pd\n",
    "course_df = pd.read_json('../common/schedule.json')\n",
    "course_df['Date'] = course_df['Lecture'].map(lambda x: x.split('-')[0])\n",
    "course_df['Title'] = course_df['Lecture'].map(lambda x: x.split('-')[-1])\n",
    "course_df[['Date', 'Title', 'Description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Overview: Segmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "course_df[['Title', 'Description', 'Applications']][3:6].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Overview: Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "course_df[['Title', 'Description', 'Applications']][6:9].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Overview: Big Imaging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "course_df[['Title', 'Description', 'Applications']][9:12].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Overview: Wrapping Up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "course_df[['Title', 'Description', 'Applications']][12:13].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Images \n",
    "\n",
    "## An introduction to images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# What is an image?\n",
    "\n",
    "----\n",
    "\n",
    "A very abstract definition: \n",
    "- __A pairing between spatial information (position)__\n",
    "- __and some other kind of information (value).__\n",
    "\n",
    "In most cases this is a 2- or 3-dimensional position (x,y,z coordinates) and a numeric value (intensity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image sampling\n",
    "<table style=\"margin-bottom:3cm\">\n",
    "<tr><td width=\"300px\">The world is</td><td width=\"400px\">The computer needs</td></tr>\n",
    "   \n",
    "<tr><td  width=\"300px\">\n",
    "\n",
    "- Continuous \n",
    "- No boundaries\n",
    "    \n",
    "</td>\n",
    "<td  width=\"300px\">\n",
    "\n",
    "- Discrete levels\n",
    "- Limited extent\n",
    "    \n",
    "</td></tr></table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"../common/figures/grid.svg\" style=\"height:400px\">\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "\n",
    "img=np.load('../common/data/wood.npy');\n",
    "plt.figure(figsize=[15,7])\n",
    "plt.subplot(2,3,1); plt.imshow(img); plt.title('Original')\n",
    "downsize =  2; plt.subplot(2,3,2); plt.imshow(resize(img,(img.shape[0] // downsize, img.shape[1] // downsize), anti_aliasing=False)); plt.title('Downsize {0}x{0}'.format(downsize))\n",
    "downsize = 32; plt.subplot(2,3,3); plt.imshow(resize(img,(img.shape[0] // downsize, img.shape[1] // downsize),anti_aliasing=False)); plt.title('Downsize {0}x{0}'.format(downsize))\n",
    "levels   = 16; plt.subplot(2,3,5); plt.imshow(np.floor(img*levels)); plt.title('{0} Levels'.format(levels));\n",
    "levels   = 4 ; plt.subplot(2,3,6); plt.imshow(np.floor(img*levels)); plt.title('{0} Levels'.format(levels));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let's create a small image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "basic_image = np.random.choice(range(100), size = (5,5))\n",
    "xx, yy = np.meshgrid(range(basic_image.shape[1]), range(basic_image.shape[0]))\n",
    "image_df = pd.DataFrame(dict(x = xx.ravel(),\n",
    "                 y = yy.ravel(),\n",
    "                 Intensity = basic_image.ravel()))\n",
    "image_df[['x', 'y', 'Intensity']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(basic_image, cmap = 'gray')\n",
    "plt.colorbar(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## 2D Intensity Images\n",
    "\n",
    "The next step is to apply a color map (also called lookup table, LUT) to the image \n",
    "- so it is a bit more exciting \n",
    "- some features are easier to detect [Rogowitz et al. 1996](https://doi.org/10.1063/1.4822401)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)\n",
    "plot_image = ax1.matshow(basic_image, cmap = 'Blues')\n",
    "plt.colorbar(plot_image)\n",
    "\n",
    "for _, c_row in image_df.iterrows():\n",
    "    ax1.text(c_row['x'], c_row['y'], s = '%02d' % c_row['Intensity'], fontdict = dict(color = 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Color maps can be arbitrarily defined based on how we would like to visualize the information in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)\n",
    "plot_image = ax1.matshow(basic_image, cmap = 'jet')\n",
    "plt.colorbar(plot_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)\n",
    "\n",
    "plot_image = ax1.matshow(basic_image, cmap = 'hot')\n",
    "plt.colorbar(plot_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Lookup Tables\n",
    "\n",
    "Formally a lookup table is a function which\n",
    "$$ f(\\textrm{Intensity}) \\rightarrow \\textrm{Color} $$\n",
    "\n",
    "#### Matplotlib's color maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "#from colorspacious import cspace_converter\n",
    "from collections import OrderedDict\n",
    "cmaps = OrderedDict() \n",
    "cmaps['Perceptually Uniform Sequential'] = [\n",
    "            'viridis', 'plasma', 'inferno', 'magma', 'cividis']\n",
    "\n",
    "cmaps['Sequential'] = [\n",
    "            'Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "            'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "            'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "\n",
    "cmaps['Sequential (2)'] = [\n",
    "            'binary', 'gist_yarg', 'gist_gray', 'gray', 'bone', 'pink',\n",
    "            'spring', 'summer', 'autumn', 'winter', 'cool', 'Wistia',\n",
    "            'hot', 'afmhot', 'gist_heat', 'copper']\n",
    "\n",
    "cmaps['Diverging'] = [\n",
    "            'PiYG', 'PRGn', 'BrBG', 'PuOr', 'RdGy', 'RdBu',\n",
    "            'RdYlBu', 'RdYlGn', 'Spectral', 'coolwarm', 'bwr', 'seismic']\n",
    "\n",
    "cmaps['Cyclic'] = ['twilight', 'twilight_shifted', 'hsv']\n",
    "\n",
    "cmaps['Qualitative'] = ['Pastel1', 'Pastel2', 'Paired', 'Accent',\n",
    "                        'Dark2', 'Set1', 'Set2', 'Set3',\n",
    "                        'tab10', 'tab20', 'tab20b', 'tab20c']\n",
    "\n",
    "cmaps['Miscellaneous'] = [\n",
    "            'flag', 'prism', 'ocean', 'gist_earth', 'terrain', 'gist_stern',\n",
    "            'gnuplot', 'gnuplot2', 'CMRmap', 'cubehelix', 'brg',\n",
    "            'gist_rainbow', 'rainbow', 'jet', 'nipy_spectral', 'gist_ncar']\n",
    "\n",
    "\n",
    "nrows = max(len(cmap_list) for cmap_category, cmap_list in cmaps.items())\n",
    "\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "\n",
    "\n",
    "def plot_color_gradients(cmap_category, cmap_list, nrows):\n",
    "    fig, axes = plt.subplots(nrows=nrows)\n",
    "    fig.subplots_adjust(top=0.95, bottom=0.01, left=0.2, right=0.99)\n",
    "    axes[0].set_title(cmap_category + ' colormaps', fontsize=14)\n",
    "\n",
    "    for ax, name in zip(axes, cmap_list):\n",
    "        ax.imshow(gradient, aspect='auto', cmap=plt.get_cmap(name))\n",
    "        pos = list(ax.get_position().bounds)\n",
    "        x_text = pos[0] - 0.01\n",
    "        y_text = pos[1] + pos[3]/2.\n",
    "        fig.text(x_text, y_text, name, va='center', ha='right', fontsize=10)\n",
    "\n",
    "    # Turn off *all* ticks & spines, not just the ones with colormaps.\n",
    "    for ax in axes:\n",
    "        ax.set_axis_off()\n",
    "\n",
    "\n",
    "for cmap_category, cmap_list in cmaps.items():\n",
    "    plot_color_gradients(cmap_category, cmap_list, nrows)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<center>Never use rainbox maps like jet, see <a href=\"https://agilescientific.com/blog/2017/12/14/no-more-rainbows\">No more rainbows!</a></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xlin = np.linspace(0, 1, 100)\n",
    "colors = ['Red','Green','Blue']\n",
    "plt.figure(figsize=[15,4])\n",
    "for i in np.arange(0,3) :\n",
    "    plt.subplot(1,3,i+1)\n",
    "\n",
    "    plt.scatter(xlin, \n",
    "            plt.cm.hot(xlin)[:,i],\n",
    "            c = plt.cm.hot(xlin),label=\"hot\")\n",
    "    plt.scatter(xlin, \n",
    "            plt.cm.Blues(xlin)[:,i], \n",
    "            c = plt.cm.Blues(xlin),label=\"blues\")\n",
    "\n",
    "    plt.scatter(xlin, \n",
    "            plt.cm.jet(xlin)[:,i], \n",
    "            c = plt.cm.jet(xlin),label='jet')\n",
    "\n",
    "    plt.xlabel('Intensity');\n",
    "    plt.ylabel('{0} Component'.format(colors[i]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applied LUTs\n",
    "These transformations can also be non-linear as is the case of the graph below where the mapping between the intensity and the color is a $\\log$ relationship meaning the the difference between the lower values is much clearer than the higher ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xlin = np.logspace(-2, 5, 500)\n",
    "log_xlin = np.log10(xlin)\n",
    "norm_xlin = (log_xlin-log_xlin.min())/(log_xlin.max()-log_xlin.min())\n",
    "fig, ax1 = plt.subplots(1,1)\n",
    "\n",
    "ax1.scatter(xlin, plt.cm.hot(norm_xlin)[:,0], \n",
    "            c = plt.cm.hot(norm_xlin))\n",
    "\n",
    "ax1.scatter(xlin, plt.cm.hot(xlin/xlin.max())[:,0], \n",
    "            c = plt.cm.hot(norm_xlin))\n",
    "ax1.set_xscale('log');ax1.set_xlabel('Intensity');ax1.set_ylabel('Red Component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "On a real image the difference is even clearer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (12, 4))\n",
    "in_img = imread('../common/figures/bone-section.png')[:,:,0].astype(np.float32)\n",
    "ax1.imshow(in_img, cmap = 'gray');\n",
    "ax1.set_title('grayscale LUT');\n",
    "\n",
    "ax2.imshow(in_img, cmap = 'hot');\n",
    "ax2.set_title('hot LUT');\n",
    "\n",
    "ax3.imshow(np.log2(in_img+1), cmap = 'gray');\n",
    "ax3.set_title('grayscale-log LUT');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## 3D Images\n",
    "\n",
    "For a 3D image, the position or spatial component has a 3rd dimension (z if it is a spatial, or t if it is a movie)\n",
    "<table><tr><td>Volume</td><td>Time series</td></tr>\n",
    "    <tr>\n",
    "    <td><img src=\"../common/figures/cube_10x10x10.svg\"></td>\n",
    "    <td><img src=\"../common/figures/timeseries_visualization.svg\"></td><tr></table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vol_image = np.arange(27).reshape((3,3,3))\n",
    "print(vol_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "This can then be rearranged from a table form into an array form and displayed as a series of slices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=10",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.util import montage as montage2d\n",
    "print(montage2d(vol_image, fill = 0))\n",
    "plt.matshow(montage2d(vol_image, fill = 0), cmap = 'jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Values\n",
    "\n",
    "In the images thus far, we have had one value per position, but there is no reason there cannot be multiple values. In fact this is what color images are (red, green, and blue) values and even 4 channels with transparency (alpha) as a different. For clarity we call the __dimensionality__ of the image the number of dimensions in the spatial position, and the __depth__ the number in the value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "base_df = pd.DataFrame([dict(x = x, y = y) for x,y in product(range(5), range(5))])\n",
    "base_df['Intensity'] = np.random.uniform(0, 1, 25)\n",
    "base_df['Transparency'] = np.random.uniform(0, 1, 25)\n",
    "base_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This can then be rearranged from a table form into an array form and displayed as a series of slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=8, fig.width = 6",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.scatter(base_df['x'], base_df['y'], c = plt.cm.gray(base_df['Intensity']), s = 1000)\n",
    "ax1.set_title('Intensity')\n",
    "ax2.scatter(base_df['x'], base_df['y'], c = plt.cm.gray(base_df['Transparency']), s = 1000)\n",
    "ax2.set_title('Transparency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "ax1.scatter(base_df['x'], base_df['y'], c = plt.cm.jet(base_df['Intensity']), s = 1000*base_df['Transparency'])\n",
    "ax1.set_title('Intensity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperspectral Imaging\n",
    "\n",
    "\n",
    "At each point in the image (black dot), instead of having just a single value, there is an entire spectrum. A selected group of these (red dots) are shown to illustrate the variations inside the sample. While certainly much more complicated, this still constitutes and image and requires the same sort of techniques to process correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "load_hypermap",
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "import os\n",
    "raw_img = imread(os.path.join('..', 'common', 'data', 'raw.jpg'))\n",
    "im_pos = pd.read_csv(os.path.join('..', 'common', 'data', 'impos.csv'), header = None)\n",
    "im_pos.columns = ['x', 'y']\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (8, 8));\n",
    "ax1.imshow(raw_img);\n",
    "ax1.scatter(im_pos['x'], im_pos['y'], s = 1, c = 'blue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(os.path.join('..', 'common', 'data', 'full_img.csv')).query('wavenum<1200')\n",
    "print(full_df.shape[0], 'rows')\n",
    "full_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "full_df['g_x'] = pd.cut(full_df['x'], 5)\n",
    "full_df['g_y'] = pd.cut(full_df['y'], 5)\n",
    "fig, m_axs = plt.subplots(5, 5, figsize = (12, 12))\n",
    "for ((g_x, g_y), c_rows), c_ax in zip(full_df.sort_values(['x','y']).groupby(['g_x', 'g_y']), m_axs.flatten()):\n",
    "    c_ax.plot(c_rows['wavenum'], c_rows['val'], 'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image Formation\n",
    "\n",
    "\n",
    "![Traditional Imaging](../common/figures/image-formation.png)\n",
    "\n",
    "- __Impulses__ Light, X-Rays, Electrons, A sharp point, Magnetic field, Sound wave\n",
    "- __Characteristics__ Electron Shell Levels, Electron Density, Phonons energy levels, Electronic, Spins, Molecular mobility\n",
    "- __Response__ Absorption, Reflection, Phase Shift, Scattering, Emission\n",
    "- __Detection__ Your eye, Light sensitive film, CCD / CMOS, Scintillator, Transducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where do images come from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "pd.read_table(StringIO(\"\"\"Modality\\tImpulse\tCharacteristic\tResponse\tDetection\n",
    "Light Microscopy\tWhite Light\tElectronic interactions\tAbsorption\tFilm, Camera\n",
    "Phase Contrast\tCoherent light\tElectron Density (Index of Refraction)\tPhase Shift\tPhase stepping, holography, Zernike\n",
    "Confocal Microscopy\tLaser Light\tElectronic Transition in Fluorescence Molecule\tAbsorption and reemission\tPinhole in focal plane, scanning detection\n",
    "X-Ray Radiography\tX-Ray light\tPhoto effect and Compton scattering\tAbsorption and scattering\tScintillator, microscope, camera\n",
    "Neutron Radiography\tNeutrons\tInteraction with nucleus\tScattering and absorption\tScintillator, optics, camera\n",
    "Ultrasound\tHigh frequency sound waves\tMolecular mobility\tReflection and Scattering\tTransducer\n",
    "MRI\tRadio-frequency EM\tUnmatched Hydrogen spins\tAbsorption and reemission\tRF coils to detect\n",
    "Atomic Force Microscopy\tSharp Point\tSurface Contact\tContact, Repulsion\tDeflection of a tiny mirror\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Acquiring Images\n",
    "\n",
    "## Traditional / Direct imaging\n",
    "- Visible images produced or can be easily made visible\n",
    "- Optical imaging, microscopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.cap=\" here the measurement is supposed to be from a typical microscope which blurs, flips and otherwise distorts the image but the original representation is still visible\"",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from scipy.ndimage import convolve\n",
    "from skimage.morphology import disk\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "bone_img = imread(os.path.join('..', 'common', 'figures', 'tiny-bone.png')).astype(np.float32)\n",
    "# simulate measured image\n",
    "conv_kern = np.pad(disk(2), 1, 'constant', constant_values = 0)\n",
    "meas_img = convolve(bone_img[::-1], conv_kern)\n",
    "# run deconvolution\n",
    "dekern = np.fft.ifft2(1/np.fft.fft2(conv_kern))\n",
    "rec_img = convolve(meas_img, dekern)[::-1]\n",
    "# show result\n",
    "fig, (ax_orig, ax1, ax2) = plt.subplots(1,3, \n",
    "                               figsize = (12, 4))\n",
    "ax_orig.imshow(bone_img, cmap = 'bone')\n",
    "ax_orig.set_title('Original Object')\n",
    "\n",
    "ax1.imshow(np.real(meas_img), cmap = 'bone')\n",
    "ax1.set_title('Measurement')\n",
    "\n",
    "ax2.imshow(np.real(rec_img), cmap = 'bone', vmin = 0, vmax = 255)\n",
    "ax2.set_title('Reconstructed');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Indirect / Computational imaging\n",
    "- Recorded information does not resemble object\n",
    "- Response must be transformed (usually computationally) to produce an image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.cap=\"here the measurement is supposed to be from a diffraction style experiment where the data is measured in reciprocal space (fourier) and can be reconstructed to the original shape\"",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from scipy.ndimage import convolve\n",
    "from skimage.morphology import disk\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "bone_img = imread(os.path.join('..', 'common', 'figures', 'tiny-bone.png')).astype(np.float32)\n",
    "# simulate measured image\n",
    "meas_img = np.log10(np.abs(np.fft.fftshift(np.fft.fft2(bone_img))))\n",
    "print(meas_img.min(), meas_img.max(), meas_img.mean())\n",
    "fig, (ax1, ax_orig) = plt.subplots(1,2, \n",
    "                               figsize = (12, 6))\n",
    "ax_orig.imshow(bone_img, cmap = 'bone')\n",
    "ax_orig.set_title('Original Object')\n",
    "\n",
    "ax1.imshow(meas_img, cmap = 'hot')\n",
    "ax1.set_title('Measurement');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Traditional Imaging\n",
    "\n",
    "\n",
    "<img src=\"../common/figures/traditional-imaging.png\" style=\"height:500px\"/>\n",
    "\n",
    "\n",
    "<small>\n",
    "Copyright 2003-2013 J. Konrad in EC520 lecture, reused with permission\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Traditional Imaging: Model\n",
    "\n",
    "\n",
    "![Traditional Imaging Model](../common/figures/traditional-image-flow.png)\n",
    "\n",
    "$$\n",
    "\\left[\\left([b(x,y)*s_{ab}(x,y)]\\otimes h_{fs}(x,y)\\right)*h_{op}(x,y)\\right]*h_{det}(x,y)+d_{dark}(x,y)\n",
    "$$\n",
    "\n",
    "$s_{ab}$ is the only information you are really interested in, so it is important to remove or correct for the other components\n",
    "\n",
    "For color (non-monochromatic) images the problem becomes even more complicated\n",
    "$$\n",
    "\\int_{0}^{\\infty} {\\left[\\left([b(x,y,\\lambda)*s_{ab}(x,y,\\lambda)]\\otimes h_{fs}(x,y,\\lambda)\\right)*h_{op}(x,y,\\lambda)\\right]*h_{det}(x,y,\\lambda)}\\mathrm{d}\\lambda+d_{dark}(x,y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Indirect Imaging (Computational Imaging)\n",
    "<table>\n",
    "<tr><td>\n",
    "    \n",
    "- Tomography through projections\n",
    "- Microlenses (Light-field photography)\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "    \n",
    "- Diffraction patterns\n",
    "- Hyperspectral imaging with Raman, IR, CARS\n",
    "- Surface Topography with cantilevers (AFM)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><video controls loop src=\"../common/movies/lightfield.mp4\" height=\"300px\" type=\"video/mp4\"></video></td>\n",
    "<td><img src=\"../common/figures/surface-plot.png\" style=\"height:300px\"/></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image Analysis\n",
    "\n",
    "\n",
    "<center><img src=\"../common/figures/approaches.png\" style=\"height:400px\"></center>\n",
    "\n",
    "\n",
    "- An image is a bucket of pixels.\n",
    "- How you choose to turn it into useful information is strongly dependent on your background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image Analysis: Experimentalist\n",
    "\n",
    "<table>\n",
    "    <tr><td align=\"left\"> \n",
    "    \n",
    "### Characteristics\n",
    "-  Problem-driven \n",
    "- Top-down \n",
    "- _Reality_ Model-based \n",
    "\n",
    "### Examples\n",
    "- cell counting\n",
    "- porosity\n",
    "</td>\n",
    "<td><img src=\"../common/figures/approaches.png\" style=\"height:400px;\"> </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image Analysis: Computer Vision Approaches\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr><td align=\"left\"> \n",
    "\n",
    "### Characteristics\n",
    "- Method-driven\n",
    " - Feature-based\n",
    " - _Image_ Model-based\n",
    "- Engineer features for solving problems\n",
    "\n",
    "### Examples\n",
    "\n",
    "- Edge detection\n",
    "- Face detection\n",
    "</td>\n",
    "<td>\n",
    "<td><img src=\"../common/figures/approaches.png\" style=\"height:400px;\"> </td>\n",
    "</td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image Analysis: Deep Learning Approach\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            \n",
    "### Characteristics\n",
    "- Results-driven\n",
    "- Biology ‘inspired’\n",
    "- Build both image processing and analysis from scratch\n",
    "\n",
    "### Examples\n",
    "\n",
    "- Captioning images\n",
    "- Identifying unusual events\n",
    "</td>\n",
    "<td><img src=\"../common/figures/approaches.png\" style=\"height:400px;\"> </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# On Science\n",
    "\n",
    "## What is the purpose?\n",
    "\n",
    "\n",
    "- Discover and validate new knowledge\n",
    "\n",
    "### How?\n",
    "- Use the scientific method as an approach to convince other people\n",
    "- Build on the results of others so we don't start from the beginning\n",
    "\n",
    "### Important Points\n",
    "- While __qualitative__ assessment is important, it is difficult to reliably produce and scale\n",
    "- __Quantitative__ analysis is far from perfect, but provides metrics which can be compared and regenerated by anyone\n",
    "\n",
    "<small>Inspired by: [imagej-pres](http://www.slideshare.net/CurtisRueden/imagej-and-the-scijava-software-stack)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Science and Imaging\n",
    "Images are great for qualitative analyses since our brains can quickly interpret them without large _programming_ investements.\n",
    "### Proper processing and quantitative analysis is however much more difficult with images.\n",
    " - If you measure a temperature, quantitative analysis is easy, $50K$.\n",
    " - If you measure an image it is much more difficult and much more prone to mistakes, subtle setup variations, and confusing analyses\n",
    "\n",
    "\n",
    "### Furthermore in image processing there is a plethora of tools available\n",
    "\n",
    "- Thousands of algorithms available\n",
    "- Thousands of tools\n",
    "- Many images require multi-step processing\n",
    "- Experimenting is time-consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why quantitative?\n",
    "\n",
    "### Human eyes have issues\n",
    "\n",
    "Which center square seems brighter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.align='center'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xlin = np.linspace(-1,1, 3)\n",
    "xx, yy = np.meshgrid(xlin, xlin)\n",
    "img_a = 25*np.ones((3,3))\n",
    "img_b = np.ones((3,3))*75\n",
    "img_a[1,1] = 50\n",
    "img_b[1,1] = 50\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 5));\n",
    "ax1.matshow(img_a, vmin = 0, vmax = 100, cmap = 'bone');\n",
    "ax2.matshow(img_b, vmin = 0, vmax = 100, cmap = 'bone');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Intensity gradients\n",
    "Are the intensities constant in the image?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xlin = np.linspace(-1,1, 10)\n",
    "xx, yy = np.meshgrid(xlin, xlin)\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (6, 6))\n",
    "ax1.matshow(xx, vmin = -1, vmax = 1, cmap = 'bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reproducibility vs. Repeatability\n",
    "\n",
    "### Reproducibility\n",
    "<img src='../common/figures/reproducibility.svg' style='height:300px'>\n",
    "\n",
    "### Repeatability\n",
    "<img src='../common/figures/repeatability.svg' style='height:300px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reproducibility vs. Repeatability\n",
    "\n",
    "Science demands __repeatability__! and really wants __reproducability__\n",
    "- Experimental conditions can change rapidly and are difficult to make consistent\n",
    "- Animal and human studies are prohibitively time consuming and expensive to reproduce\n",
    "- Terabyte datasets cannot be easily passed around many different groups\n",
    "- Privacy concerns can also limit sharing and access to data\n",
    "\n",
    "----\n",
    "\n",
    "- _Science_ is already difficult enough\n",
    "- Image processing makes it even more complicated\n",
    "- Many image processing tasks are multistep, have many parameters, use a variety of tools, and consume a very long time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How can we keep track of everything for ourselves and others?\n",
    "- We can make the data analysis easy to repeat by an independent 3rd party\n",
    "- Document the analysis steps\n",
    "- Write clear and understandable code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computing has changed: Parallel\n",
    "\n",
    "\n",
    "## Moores Law\n",
    "$$ \\textrm{Transistors} \\propto 2^{T/(\\textrm{18 months})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# stolen from https://gist.github.com/humberto-ortiz/de4b3a621602b78bf90d\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "moores_txt=[\"Id Name  Year  Count(1000s)  Clock(MHz)\\n\",\n",
    "        \"0            MOS65XX  1975           3.51           14\\n\",\n",
    "        \"1          Intel8086  1978          29.00           10\\n\",\n",
    "        \"2          MIPSR3000  1988         120.00           33\\n\",\n",
    "        \"3           AMDAm486  1993        1200.00           40\\n\",\n",
    "        \"4        NexGenNx586  1994        3500.00          111\\n\",\n",
    "        \"5          AMDAthlon  1999       37000.00         1400\\n\",\n",
    "        \"6   IntelPentiumIII  1999       44000.00         1400\\n\",\n",
    "        \"7         PowerPC970  2002       58000.00         2500\\n\",\n",
    "        \"8       AMDAthlon64  2003      243000.00         2800\\n\",\n",
    "        \"9    IntelCore2Duo  2006      410000.00         3330\\n\",\n",
    "        \"10         AMDPhenom  2007      450000.00         2600\\n\",\n",
    "        \"11      IntelCorei7  2008     1170000.00         3460\\n\",\n",
    "        \"12      IntelCorei5  2009      995000.00         3600\"]\n",
    "\n",
    "sio_table = StringIO(''.join(moores_txt)); moore_df = pd.read_table(sio_table, sep = '\\s+', index_col = 0);\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (8, 4)); ax1.semilogy(moore_df['Year'], moore_df['Count(1000s)'], 'b.-', label = '1000s of transitiors'); ax1.semilogy(moore_df['Year'], moore_df['Clock(MHz)'], 'r.-', label = 'Clockspeed (MHz)') ;ax1.legend(loc = 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<small>_Based on data from https://gist.github.com/humberto-ortiz/de4b3a621602b78bf90d_</small>\n",
    "\n",
    "----\n",
    "\n",
    "There are now many more transistors inside a single computer but the processing speed hasn't increased. How can this be?\n",
    "\n",
    "- Multiple Core\n",
    " - Many machines have multiple cores for each processor which can perform tasks independently\n",
    "- Multiple CPUs\n",
    " - More than one chip is commonly present\n",
    "- New modalities\n",
    "  - GPUs provide many cores which operate at slow speed\n",
    "\n",
    "### $\\rightarrow$ Parallel Code is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing has changed: Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table style=\"background-color: white;\">\n",
    "    <tr>\n",
    "    <td>\n",
    "        \n",
    "- Computer, servers, workstations are _wildly underused_ (majority are <50%)\n",
    "- Buying a big computer that sits _idle most of the time_ is a waste of money\n",
    "\n",
    "    - http://www-inst.eecs.berkeley.edu/~cs61c/sp14/\n",
    "    - “The Case for Energy-Proportional Computing,” Luiz André Barroso, Urs Hölzle, IEEE Computer, December 2007\n",
    "\n",
    "- Traditionally the most important performance criteria was time, how fast can it be done\n",
    "- With Platform as a service servers can be _rented instead of bought_\n",
    "- Speed is still important but using cloud computing $ / Sample is the real metric\n",
    "- In Switzerland a PhD student is 400x as expensive per hour as an Amazon EC2 Machine\n",
    "- Many competitors keep prices low and offer flexibility\n",
    "</td>\n",
    "<td><img src=\"../common/figures/cloud-services.png\" style=\"height:600\"></td>\n",
    "</tr><table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Cloud Computing Costs\n",
    "\n",
    "\n",
    "The figure shows the range of cloud costs (determined by peak usage) compared to a local workstation with utilization shown as the average number of hours the computer is used each week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "## Cloud: Equal Cost Point\n",
    "\n",
    "Here the equal cost point is shown where the cloud and local workstations have the same cost. The x-axis is the percentage of resources used at peak-time and the y shows the expected usable lifetime of the computer. The color indicates the utilization percentage and the text on the squares shows this as the numbers of hours used in a week.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Soup/Recipe Example\n",
    "\n",
    "## Simple Soup\n",
    "Easy to follow the list, anyone with the right steps can execute and repeat (if not reproduce) the soup\n",
    "\n",
    "1. Buy {carrots, peas, tomatoes} at market\n",
    "1. _then_ Buy meat at butcher\n",
    "1. _then_ Chop carrots into pieces\n",
    "1. _then_ Chop potatos into pieces\n",
    "1. _then_ Heat water\n",
    "1. _then_ Wait until boiling then add chopped vegetables\n",
    "1. _then_ Wait 5 minutes and add meat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## More complicated soup\n",
    "Here it is harder to follow and you need to carefully keep track of what is being performed\n",
    "\n",
    "### Steps 1-4\n",
    "4. _then_ Mix carrots with potatos $\\rightarrow  mix_1$\n",
    "4. _then_ add egg to $mix_1$ and fry for 20 minutes\n",
    "4. _then_ Tenderize meat for 20 minutes\n",
    "4. _then_ add tomatoes to meat and cook for 10 minutes $\\rightarrow mix_2$\n",
    "5. _then_ Wait until boiling then add $mix_1$\n",
    "6. _then_ Wait 5 minutes and add $mix_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using flow charts / workflows\n",
    "\n",
    "## Simple Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "import pydot\n",
    "graph = pydot.Dot(graph_type='digraph', rankdir=\"LR\")\n",
    "node_names = [\"Buy\\nvegetables\",\"Buy meat\",\"Chop\\nvegetables\",\"Heat water\", \"Add Vegetables\",\n",
    "              \"Wait for\\nboiling\",\"Wait 5\\nadd meat\"]\n",
    "nodes = [pydot.Node(name = '%04d' % i, label = c_n) \n",
    "         for i, c_n in enumerate(node_names)]\n",
    "for c_n in nodes:\n",
    "    graph.add_node(c_n)\n",
    "    \n",
    "for (c_n, d_n) in zip(nodes, nodes[1:]):\n",
    "    graph.add_edge(pydot.Edge(c_n, d_n))\n",
    "\n",
    "SVG(graph.create_svg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Workflows\n",
    "\n",
    "Clearly a linear set of instructions is ill-suited for even a fairly easy soup, it is then even more difficult when there are dozens of steps and different pathsways\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "Furthermore a clean workflow allows you to better parallelize the task since it is clear which tasks can be performed independently\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=9",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "import pydot\n",
    "graph = pydot.Dot(graph_type='digraph', rankdir=\"LR\")\n",
    "node_names = [\"Buy\\nvegetables\",\"Buy meat\",\"Chop\\nvegetables\",\"Heat water\", \"Add Vegetables\",\n",
    "              \"Wait for\\nboiling\",\"Wait 5\\nadd meat\"]\n",
    "nodes = [pydot.Node(name = '%04d' % i, label = c_n, style = 'filled') \n",
    "         for i, c_n in enumerate(node_names)]\n",
    "for c_n in nodes:\n",
    "    graph.add_node(c_n)\n",
    "    \n",
    "def e(i,j, col = None):\n",
    "    if col is not None:\n",
    "        for c in [i,j]:\n",
    "            if nodes[c].get_fillcolor() is None: \n",
    "                nodes[c].set_fillcolor(col)\n",
    "    graph.add_edge(pydot.Edge(nodes[i], nodes[j]))\n",
    "\n",
    "e(0, 2, 'gold'); e(2, 4); e(3, -2, 'springgreen'); e(-2, 4, 'orange'); e(4, -1) ; e(1, -1, 'dodgerblue')\n",
    "\n",
    "SVG(graph.create_svg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Directed Acyclical Graphs (DAG)\n",
    "We can represent almost any computation without loops as DAG. What this allows us to do is now break down a computation into pieces which can be carried out independently. There are a number of tools which let us handle this issue.\n",
    "\n",
    "- PyData Dask - https://dask.pydata.org/en/latest/\n",
    "- Apache Spark - https://spark.apache.org/\n",
    "- Spotify Luigi - https://github.com/spotify/luigi\n",
    "- Airflow - https://airflow.apache.org/\n",
    "- KNIME - https://www.knime.com/\n",
    "- Google Tensorflow - https://www.tensorflow.org/\n",
    "- Pytorch / Torch - http://pytorch.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Concrete example\n",
    "What is a DAG good for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "from dask.dot import dot_graph\n",
    "image_1 = da.zeros((5,5), chunks = (5,5))\n",
    "image_2 = da.ones((5,5), chunks = (5,5))\n",
    "dot_graph(image_1.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "image_3 = image_1 + image_2\n",
    "dot_graph(image_3.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "image_4 = (image_1-10) + (image_2*50)\n",
    "dot_graph(image_4.dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's go big\n",
    "Now let's see where this can be really useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.dot import dot_graph\n",
    "image_1 = da.zeros((1024, 1024), chunks = (512, 512))\n",
    "image_2 = da.ones((1024 ,1024), chunks = (512, 512))\n",
    "dot_graph(image_1.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "image_4 = (image_1-10) + (image_2*50)\n",
    "dot_graph(image_4.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "image_5 = da.matmul(image_1, image_2)\n",
    "dot_graph(image_5.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "image_6 = (da.matmul(image_1, image_2)+image_1)*image_2\n",
    "dot_graph(image_6.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import dask_ndfilters as da_ndfilt\n",
    "image_7 = da_ndfilt.convolve(image_6, image_1)\n",
    "dot_graph(image_7.dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Deep Learning\n",
    "We won't talk too much about deep learning now, but it certainly shows why DAGs are so important. \n",
    "\n",
    "The steps above are simple toys compared to what tools are already in use for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.utils.vis_utils import model_to_dot \n",
    "resnet = ResNet50(weights = None)\n",
    "SVG(model_to_dot(resnet).create_svg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = (\"<stripped %d bytes>\"%size).encode('ascii')\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "sess = K.get_session()\n",
    "show_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "livereveal": {
   "autolaunch": true,
   "footer": "February 20, 2020 - ETH 227-0966-00L: Quantitative Big Imaging - Introduction",
   "header": "<table width='100%' style='margin: 0px;'><tr><td align='left'><img src='../common/figures/eth_logo_kurz_pos.svg' style='height:30px;'></td><td align='right'><img src='../common/figures/PSI-Logo.svg' style='height:50px;'></td></tr></table>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
