{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ETHZ: 227-0966-00L\n",
    "# Quantitative Big Imaging\n",
    "# April 9, 2020\n",
    "\n",
    "## Dynamic Experiments: Introduction\n",
    "\n",
    "### Anders Kaestner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams['font.family'] = ['sans-serif']\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Literature / Useful References\n",
    "\n",
    "### Books\n",
    "- Jean Claude, Morphometry with R\n",
    " - [Online](http://link.springer.com/book/10.1007%2F978-0-387-77789-4) through ETHZ\n",
    " - [Buy it](http://www.amazon.com/Morphometrics-R-Use-Julien-Claude/dp/038777789X)\n",
    "- John C. Russ, “The Image Processing Handbook”,(Boca Raton, CRC Press)\n",
    " - Available [online](http://dx.doi.org/10.1201/9780203881095) within domain ethz.ch (or proxy.ethz.ch / public VPN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Papers / Sites\n",
    "\n",
    "- Comparsion of Tracking Methods in Biology\n",
    " - Chenouard, N., Smal, I., de Chaumont, F., Maška, M., Sbalzarini, I. F., Gong, Y., … Meijering, E. (2014). Objective comparison of particle tracking methods. Nature Methods, 11(3), 281–289. doi:10.1038/nmeth.2808\n",
    " - Maska, M., Ulman, V., Svoboda, D., Matula, P., Matula, P., Ederra, C., … Ortiz-de-Solorzano, C. (2014). A benchmark for comparison of cell tracking algorithms. Bioinformatics (Oxford, England), btu080–. doi:10.1093/bioinformatics/btu080\n",
    "- Keypoint and Corner Detection\n",
    " - Distinctive Image Features from Scale-Invariant Keypoints - https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\n",
    " - https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html\n",
    "- Registration\n",
    " - https://itk.org/ITKSoftwareGuide/html/Book2/ITKSoftwareGuide-Book2ch3.html\n",
    "- Multiple Hypothesis Testing\n",
    " - Coraluppi, S. & Carthel, C. Multi-stage multiple-hypothesis tracking.\n",
    "J. Adv. Inf. Fusion 6, 57–67 (2011).\n",
    " - Chenouard, N., Bloch, I. & Olivo-Marin, J.-C. Multiple hypothesis\n",
    "tracking in microscopy images. in Proc. IEEE Int. Symp. Biomed. Imaging\n",
    "1346–1349 (IEEE, 2009)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "# Previously on QBI ...\n",
    "\n",
    "- Image Enhancment \n",
    " - Highlighting the contrast of interest in images\n",
    " - Minimizing Noise\n",
    "- Understanding image histograms\n",
    "- Automatic Methods\n",
    "- Component Labeling\n",
    "- Single Shape Analysis\n",
    "- Complicated Shapes\n",
    "- Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quantitative \"Big\" Imaging\n",
    "\n",
    "\n",
    "The course has covered imaging enough and there have been a few quantitative metrics, but \"big\" has not really entered.\n",
    "\n",
    "What does __big__ mean?\n",
    "\n",
    "- Not just / even large\n",
    "- it means being ready for _big data_\n",
    "- The three V's\n",
    "    - volume, \n",
    "    - velocity, \n",
    "    - variety \n",
    "- scalable, fast, easy to customize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So what is \"big\" imaging\n",
    "\n",
    "- doing analyses in a disciplined manner\n",
    " - fixed steps\n",
    " - easy to regenerate results\n",
    " - no _magic_\n",
    "- having everything automated\n",
    " - 100 samples is as easy as 1 sample\n",
    "- being able to adapt and reuse analyses\n",
    " - one really well working script and modify parameters\n",
    " - different types of cells\n",
    " - different regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives\n",
    "\n",
    "## Experiments\n",
    "1. What sort of dynamic experiments do we have?\n",
    "1. How can we design good dynamic experiments?\n",
    "\n",
    "## Image analysis\n",
    "1. How can we track objects between points?\n",
    "1. How can we track shape?\n",
    "1. How can we track distribution?\n",
    "1. How can we track topology?\n",
    "1. How can we track voxels?\n",
    "1. How can we assess deformation and strain?\n",
    "1. How can assess more general cases?\n",
    "\n",
    "## $\\rightarrow$ How does this help answering your questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "- Motivation (Why and How?)\n",
    "- Scientific Goals\n",
    "\n",
    "## Experiments\n",
    "\n",
    "- Simulations\n",
    "- Experiment Design\n",
    " \n",
    "## Object Tracking\n",
    "### Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pixel/Voxel-based Methods\n",
    " - Cross Correlation\n",
    " - DIC\n",
    " - DIC + Physics\n",
    " - Affine Tranfroms\n",
    " - Non-rigid transform\n",
    "\n",
    "### Keypoint Detection\n",
    "- Corner Detectors\n",
    "- SIFT/SURF\n",
    "- Tracking from Keypoints\n",
    " \n",
    "## General Problems\n",
    " - Thickness - Lung Tissue\n",
    " - Curvature - Metal Systems\n",
    " - Two Point Correlation - Volcanic Rock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation\n",
    "\n",
    "- 3D images are already difficult to interpret on their own\n",
    "- 3D movies (4D) are almost impossible \n",
    "\n",
    "<video controls loop src=\"../common/movies/dk31_foam.mp4\" height=\"300px\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2D movies can also be challenging\n",
    "\n",
    "They are 3D - x,y,t\n",
    "\n",
    "### Example: a water jet\n",
    "<video controls loop src=\"ext-figures/WaterJet.m4v\" height=\"100px\" type=\"video/mp4\"></video>\n",
    "\n",
    "### Example: [coffee making](https://youtu.be/VESMU7JfVHU)\n",
    "<img src=\"../common/figures/MokaTimeSeries.png\" style=\"height:300px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What information are you looking for?\n",
    "We can say that it looks like, but many pieces of quantitative information are difficult to extract\n",
    "- How fast is it going?\n",
    "- How many particles are present?\n",
    "- Are their sizes constant?\n",
    "- Are some moving faster?\n",
    "- Are they rearranging?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3D Reconstruction\n",
    "\n",
    "### Tomography\n",
    "One of the most commonly used scans in the hospital is called a computed tomography scan. \n",
    "\n",
    "This scan works by creating 2D X-ray projections in a number of different directions in order to determine what the 3D volume looks like\n",
    "<img src=\"ext-figures/ct_scan.gif\" style=\"height:300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stage Tilting\n",
    "Beyond just tracking we can take multiple frames of a still image and instead of looking for changes in the object, we can change the angle. The pollen image below shows this for SEM\n",
    "![Pollen](ext-figures/pollen.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scientific Goals of dynamic experiments\n",
    "\n",
    "### Rheology\n",
    "\n",
    "Understanding the flow of liquids and mixtures is important for many processes\n",
    "- blood movement in arteries, veins, and capillaries\n",
    "- oil movement through porous rock\n",
    "- air through dough when cooking bread\n",
    "- magma and gas in a volcano\n",
    "\n",
    "\n",
    "### Deformation\n",
    "\n",
    "Deformation is similarly important since it plays a significant role in the following scenarios\n",
    "\n",
    "- red blood cell lysis in artificial heart valves\n",
    "- microfractures growing into stress fractures in bone\n",
    "- toughening in certain wood types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiments\n",
    "The first step of any of these analyses is proper experimental design. \n",
    "Since there is always:\n",
    "\n",
    "- A limited field of view\n",
    "- A voxel size\n",
    "- A maximum rate of measurements\n",
    "- Dose limitations\n",
    "    - Sample damage\n",
    "    - Limited flux\n",
    "- A non-zero cost for each measurement\n",
    "\n",
    "There are always trade-offs to be made between getting the best possible high-resolution nanoscale dynamics and capturing the system level behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Planning dynamic experiments\n",
    "<table>\n",
    "<tr><td>\n",
    "    \n",
    "#### If we measure too fast\n",
    " - sample damage\n",
    " - miss out on long term changes\n",
    " - have noisy data\n",
    " \n",
    "#### Too slow\n",
    " - miss small, rapid changes\n",
    " - blurring and other motion artifacts\n",
    " \n",
    "#### Too high resolution\n",
    " - not enough unique structures in field of view to track\n",
    " \n",
    "#### Too low resolution\n",
    " - not sensitive to small changes\n",
    "    \n",
    "</td><td>\n",
    "<img src=\"../common/figures/DynamicExperimentsTradeOffs.png\" style=\"height:600px\"/>    \n",
    "</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tuning experiment by simulation\n",
    "\n",
    "#### In many cases: \n",
    "- experimental data is inherited \n",
    "- little can be done about the design, \n",
    "\n",
    "#### When there is still the opportunity to tune the experiment\n",
    "simulations provide a powerful tool for tuning and balancing a large number parameters\n",
    "\n",
    "### Validation\n",
    "Simulations also provide the ability to pair post-processing to the experiments and determine the limits of tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What do we start with?\n",
    "\n",
    "Going back to our original cell image\n",
    "\n",
    "1. We have been able to _get rid of the noise_ in the image and _find all the cells_ (__lecture 2-4__)\n",
    "1. We have analyzed the shape of the cells using the shape tensor (__lecture 5__)\n",
    "1. We even _separated cells_ joined together using Watershed (__lecture 6__)\n",
    "1. We have created even more _metrics characterizing the distribution_ (__lecture 7__)\n",
    "\n",
    "We have at least a few samples (or different regions), large number of metrics and an almost as large number of parameters to _tune_\n",
    "\n",
    "\n",
    "### How do we do something meaningful with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic Simulation\n",
    "\n",
    "\n",
    "We start with a starting image with a number of circles on a plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "%matplotlib inline\n",
    "xx, yy = np.meshgrid(np.linspace(-1.5, 1.5, 15),\n",
    "                     np.linspace(-1.5, 1.5, 15))\n",
    "\n",
    "N_DISK_ROW = 2\n",
    "N_DISK_COL = 4\n",
    "DISK_RAD = 0.15\n",
    "disk_img = np.zeros(xx.shape, dtype=int)\n",
    "for x_cent in 0.7*np.linspace(-1, 1, N_DISK_COL):\n",
    "    for y_cent in 0.7*np.linspace(-1, 1, N_DISK_ROW):\n",
    "        c_disk = np.sqrt(np.square(xx-x_cent)+np.square(yy-y_cent)) < DISK_RAD\n",
    "        disk_img[c_disk] = 1\n",
    "fig, ax1 = plt.subplots(1, 1)\n",
    "\n",
    "sns.heatmap(disk_img, annot=True, fmt='d', ax=ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Create a series of moving \"cells\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "fig, c_ax = plt.subplots(1, 1, figsize=(5, 5), dpi=100)\n",
    "\n",
    "s_img = disk_img.copy()\n",
    "img_list = [s_img]\n",
    "for i in range(4):\n",
    "    s_img = np.roll(s_img, -1, axis=1)\n",
    "    s_img = np.roll(s_img, -1, axis=0)\n",
    "    img_list += [s_img]\n",
    "\n",
    "def update_frame(i):\n",
    "    plt.cla()\n",
    "    sns.heatmap(img_list[i],annot=True,\n",
    "                fmt=\"d\",cmap='nipy_spectral',\n",
    "                ax=c_ax,cbar=False,\n",
    "                vmin=0,vmax=1)\n",
    "    c_ax.set_title('Iteration #{}'.format(i+1))\n",
    "\n",
    "# write animation frames\n",
    "anim_code = FuncAnimation(fig,\n",
    "                          update_frame,\n",
    "                          frames=len(img_list),\n",
    "                          interval=1000, repeat_delay=2000).to_html5_video()\n",
    "plt.close('all')\n",
    "HTML(anim_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Analysis\n",
    "The analysis of the series requires the following steps:\n",
    "1. Threshold\n",
    "2. Component Label\n",
    "3. Shape Analysis\n",
    "4. Distribution Analysis\n",
    "\n",
    "... and to put all in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.morphology import label\n",
    "from skimage.measure import regionprops\n",
    "import pandas as pd\n",
    "all_objs = []\n",
    "for frame_idx, c_img in enumerate(img_list):       # For each time frame\n",
    "    lab_img = label(c_img > 0)                     # Label\n",
    "    for c_obj in regionprops(lab_img):             # Put region properties for each object of the time frame\n",
    "        all_objs += [dict(label=int(c_obj.label),  \n",
    "                          y=c_obj.centroid[0],\n",
    "                          x=c_obj.centroid[1],\n",
    "                          area=c_obj.area,\n",
    "                          frame_idx=frame_idx)]\n",
    "        \n",
    "all_obj_df = pd.DataFrame(all_objs)                # Create a Pandas data frame with all the properties\n",
    "all_obj_df.head(5)                                 # Look at the first five rows of the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Looking at the positions of the items in all frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, c_ax = plt.subplots(1, 1, figsize=(5, 5), dpi=150)\n",
    "c_ax.imshow(disk_img, cmap='bone_r')\n",
    "for frame_idx, c_rows in all_obj_df.groupby('frame_idx'):\n",
    "    c_ax.plot(c_rows['x'], c_rows['y'], 's', label='Frame: %d' % frame_idx)\n",
    "c_ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Describing Motion\n",
    "We can describe the motion in the above example with a simple vector\n",
    "\n",
    "$$ \\vec{v}(\\vec{x})=\\langle -1,-1 \\rangle $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Scoring Tracking\n",
    "\n",
    "In the following examples we will use simple metrics for scoring fits where the objects are matched and the number of misses is counted.\n",
    "\n",
    "There are a number of more sensitive scoring metrics which can be used, \n",
    "by finding the best submatch for a given particle since the number of matches and particles does not always correspond. \n",
    "\n",
    "See the papers at the beginning for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Tracking methods\n",
    "\n",
    "While there exist a number of different methods and complicated approaches for tracking. \n",
    "\n",
    "For experimental design it is best to start with the \n",
    "- simplest\n",
    "- easiest understood \n",
    "method. \n",
    "\n",
    "The limits of this can be found and components added as needed until it is possible to realize the experiment\n",
    "\n",
    "---\n",
    "If a dataset can only be analyzed with a multiple-hypothesis testing neural network model then it might not be so reliable\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Tracking using Nearest Neighbor\n",
    "\n",
    "We then return to _nearest neighbor_ which means we track \n",
    "- a point ($\\vec{P}_0$) from an image ($I_0$) at $t_0$ \n",
    "- to a point ($\\vec{P}_1$) in image ($I_1$) at $t_1$ \n",
    "\n",
    "by \n",
    "\n",
    "$$ \\vec{P}_1=\\textrm{argmin}(||\\vec{P}_0-\\vec{y}||  \\;\\forall \\vec{y}\\in I_1) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "column",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "frame_0 = all_obj_df[all_obj_df['frame_idx'].isin([0])]\n",
    "frame_1 = all_obj_df[all_obj_df['frame_idx'].isin([1])]\n",
    "fig, c_ax = plt.subplots(1, 1, figsize=(5, 5), dpi=150)\n",
    "c_ax.matshow(1 < disk_img , cmap='gist_yarg')\n",
    "c_ax.scatter(frame_0['x'], frame_0['y'], c='black', label='Frame: 0')\n",
    "c_ax.scatter(frame_1['x'], frame_1['y'], c='red', label='Frame: 1')\n",
    "dist_df_list = []\n",
    "for _, row_0 in frame_0.iterrows():\n",
    "    for _, row_1 in frame_1.iterrows():\n",
    "        seg_dist = np.sqrt(np.square(row_0['x']-row_1['x']) +\n",
    "                           np.square(row_0['y']-row_1['y']))\n",
    "        c_ax.plot([row_0['x'], row_1['x']],\n",
    "                  [row_0['y'], row_1['y']], '-', alpha=1/seg_dist)\n",
    "        dist_df_list += [dict(x0=row_0['x'],\n",
    "                              y0=row_0['y'],\n",
    "                              lab0=int(row_0['label']),\n",
    "                              x1=row_1['x'],\n",
    "                              y1=row_1['y'],\n",
    "                              lab1=int(row_1['label']),\n",
    "                              dist=seg_dist)]\n",
    "c_ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Put the distances in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "column",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dist_df = pd.DataFrame(dist_df_list)\n",
    "dist_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig, c_ax = plt.subplots(1, 1, figsize=(5, 5), dpi=150)\n",
    "c_ax.matshow(disk_img > 1, cmap='gist_yarg')\n",
    "c_ax.scatter(frame_0['x'], frame_0['y'], c='black', label='Frame: 0')\n",
    "c_ax.scatter(frame_1['x'], frame_1['y'], c='red', label='Frame: 1')\n",
    "\n",
    "def update_frame(i):\n",
    "    # plt.cla()\n",
    "    c_rows = dist_df.query('lab0==%d' % i)\n",
    "    for _, c_row in c_rows.iterrows():\n",
    "        c_ax.quiver(c_row['x0'], c_row['y0'],\n",
    "                    c_row['x1']-c_row['x0'],\n",
    "                    c_row['y1']-c_row['y0'], scale=1.0, scale_units='xy', angles='xy',\n",
    "                    alpha=1/c_row['dist'])\n",
    "    c_ax.set_title('Point #{}'.format(i+1))\n",
    "\n",
    "\n",
    "\n",
    "# write animation frames\n",
    "anim_code = FuncAnimation(fig,\n",
    "                          update_frame,\n",
    "                          frames=np.unique(dist_df['lab0']),\n",
    "                          interval=1000,\n",
    "                          repeat_delay=2000).to_html5_video()\n",
    "plt.close('all')\n",
    "HTML(anim_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, c_ax = plt.subplots(1, 1, figsize=(5, 5), dpi=150)\n",
    "c_ax.matshow(disk_img > 1,cmap='gist_yarg')\n",
    "c_ax.scatter(frame_0['x'], frame_0['y'], c='black', label='Frame: 0')\n",
    "c_ax.scatter(frame_1['x'], frame_1['y'], c='red', label='Frame: 1')\n",
    "for _, c_rows in dist_df.groupby('lab0'):\n",
    "    _, c_row = next(c_rows.sort_values('dist').iterrows())\n",
    "    c_ax.quiver(c_row['x0'], c_row['y0'],\n",
    "                c_row['x1']-c_row['x0'],\n",
    "                c_row['y1']-c_row['y0'],\n",
    "                scale=1.0, scale_units='xy', angles='xy')\n",
    "    \n",
    "c_ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig, c_ax = plt.subplots(1, 1, figsize=(5, 5), dpi=150)\n",
    "c_ax.matshow(disk_img > 1,cmap='gist_yarg')\n",
    "\n",
    "\n",
    "def draw_timestep(i):\n",
    "    # plt.cla()\n",
    "    frame_0 = all_obj_df[all_obj_df['frame_idx'].isin([i])]\n",
    "    frame_1 = all_obj_df[all_obj_df['frame_idx'].isin([i+1])]\n",
    "    c_ax.scatter(frame_0['x'], frame_0['y'], c='black', label='Frame: %d' % i)\n",
    "    c_ax.scatter(frame_1['x'], frame_1['y'],\n",
    "                 c='red', label='Frame: %d' % (i+1))\n",
    "    dist_df_list = []\n",
    "    for _, row_0 in frame_0.iterrows():\n",
    "        for _, row_1 in frame_1.iterrows():\n",
    "            dist_df_list += [dict(x0=row_0['x'],\n",
    "                                  y0=row_0['y'],\n",
    "                                  lab0=int(row_0['label']),\n",
    "                                  x1=row_1['x'],\n",
    "                                  y1=row_1['y'],\n",
    "                                  lab1=int(row_1['label']),\n",
    "                                  dist=np.sqrt(\n",
    "                                      np.square(row_0['x']-row_1['x']) +\n",
    "                                      np.square(row_0['y']-row_1['y'])))]\n",
    "    dist_df = pd.DataFrame(dist_df_list)\n",
    "    for _, c_rows in dist_df.groupby('lab0'):\n",
    "        _, best_row = next(c_rows.sort_values('dist').iterrows())\n",
    "        c_ax.quiver(best_row['x0'], best_row['y0'],\n",
    "                    best_row['x1']-best_row['x0'],\n",
    "                    best_row['y1']-best_row['y0'],\n",
    "                    scale=1.0, scale_units='xy', angles='xy')\n",
    "    c_ax.set_title('Frame #{}'.format(i+1))\n",
    "\n",
    "\n",
    "# write animation frames\n",
    "anim_code = FuncAnimation(fig,\n",
    "                          draw_timestep,\n",
    "                          frames=all_obj_df['frame_idx'].max(),\n",
    "                          interval=1000,\n",
    "                          repeat_delay=2000).to_html5_video()\n",
    "plt.close('all')\n",
    "HTML(anim_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Computing Average Flow\n",
    "From each of these time steps we can now proceed to compute the average flow. \n",
    "\n",
    "We can perform this :\n",
    "- spatially (averaging over regions and time), \n",
    "- temporally (averaging over regions), \n",
    "- or spatial-temporally (averaging over regions for every time step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "average_field = []\n",
    "for i in range(all_obj_df['frame_idx'].max()):\n",
    "    frame_0 = all_obj_df[all_obj_df['frame_idx'].isin([i])]\n",
    "    frame_1 = all_obj_df[all_obj_df['frame_idx'].isin([i+1])]\n",
    "    dist_df_list = []\n",
    "    for _, row_0 in frame_0.iterrows():\n",
    "        for _, row_1 in frame_1.iterrows():\n",
    "            dist_df_list += [dict(x0=row_0['x'],\n",
    "                                  y0=row_0['y'],\n",
    "                                  lab0=int(row_0['label']),\n",
    "                                  x1=row_1['x'],\n",
    "                                  y1=row_1['y'],\n",
    "                                  lab1=int(row_1['label']),\n",
    "                                  dist=np.sqrt(\n",
    "                                      np.square(row_0['x']-row_1['x']) +\n",
    "                                      np.square(row_0['y']-row_1['y'])))]\n",
    "    dist_df = pd.DataFrame(dist_df_list)\n",
    "    for _, c_rows in dist_df.groupby('lab0'):\n",
    "        _, best_row = next(c_rows.sort_values('dist').iterrows())\n",
    "        average_field += [dict(frame_idx=i,\n",
    "                               x=best_row['x0'],\n",
    "                               y=best_row['y0'],\n",
    "                               dx=best_row['x1']-best_row['x0'],\n",
    "                               dy=best_row['y1']-best_row['y0'])]\n",
    "average_field_df = pd.DataFrame(average_field)\n",
    "print('Average Flow:')\n",
    "average_field_df[['dx', 'dy']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Spatially Averaging\n",
    "To spatially average we first create a grid of values and then interpolate our results onto this grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "column",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp2d\n",
    "\n",
    "\n",
    "def img_intp(f):\n",
    "    def new_f(x, y):\n",
    "        return np.stack([f(ix, iy) for ix, iy in zip(np.ravel(x), np.ravel(y))], 0).reshape(np.shape(x))\n",
    "    return new_f\n",
    "\n",
    "\n",
    "dx_func = img_intp(\n",
    "    interp2d(average_field_df['x'], average_field_df['y'], average_field_df['dx']))\n",
    "dy_func = img_intp(\n",
    "    interp2d(average_field_df['x'], average_field_df['y'], average_field_df['dy']))\n",
    "dx_func(8, 8), dy_func(8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "g_x, g_y = np.meshgrid(np.linspace(average_field_df['x'].min(),\n",
    "                                   average_field_df['x'].max(), 10),\n",
    "                       np.linspace(average_field_df['y'].min(),\n",
    "                                   average_field_df['y'].max(), 10))\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(24, 4))\n",
    "sns.heatmap(g_x, ax=ax1, annot=True)\n",
    "ax1.set_title('X Position')\n",
    "sns.heatmap(g_y, ax=ax2, annot=True)\n",
    "ax2.set_title('Y Position')\n",
    "\n",
    "sns.heatmap(dx_func(g_x, g_y), ax=ax3, annot=True)\n",
    "ax3.set_title('$\\Delta x$')\n",
    "sns.heatmap(dy_func(g_x, g_y), ax=ax4, annot=True)\n",
    "ax4.set_title('$\\Delta y$')\n",
    "ax5.quiver(g_x, g_y, dx_func(g_x, g_y), dy_func(g_x, g_y),\n",
    "           scale=1.0, scale_units='xy', angles='xy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Temporarly Averaging\n",
    "Here we take the average at each time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "temp_avg_field = average_field_df[['frame_idx', 'dx', 'dy']].groupby(\n",
    "    'frame_idx').agg('mean').reset_index()\n",
    "temp_avg_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 4))\n",
    "ax1.plot(temp_avg_field['frame_idx'], temp_avg_field['dx'], 'rs-')\n",
    "ax1.set_title('$\\Delta x$')\n",
    "ax1.set_xlabel('Timestep')\n",
    "ax2.plot(temp_avg_field['frame_idx'], temp_avg_field['dy'], 'rs-')\n",
    "ax2.set_title('$\\Delta y$')\n",
    "ax2.set_xlabel('Timestep')\n",
    "ax3.quiver(temp_avg_field['dx'], temp_avg_field['dy'],\n",
    "           scale=1, scale_units='xy', angles='xy')\n",
    "ax3.set_title('$\\Delta x$, $\\Delta y$')\n",
    "ax3.set_xlabel('Timestep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Spatiotemporal Relationship\n",
    "We can also divide the images into space and time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "g_x, g_y = np.meshgrid(np.linspace(average_field_df['x'].min(),\n",
    "                                   average_field_df['x'].max(), 4),\n",
    "                       np.linspace(average_field_df['y'].min(),\n",
    "                                   average_field_df['y'].max(), 4))\n",
    "\n",
    "frames = len(sorted(np.unique(average_field_df['frame_idx'])))\n",
    "fig, m_axs = plt.subplots(2, 3, figsize=(14, 10))\n",
    "for c_ax in m_axs.flatten():\n",
    "    c_ax.axis('off')\n",
    "[(ax1, ax2, _), (ax3, ax4, ax5)] = m_axs\n",
    "\n",
    "\n",
    "def draw_frame_idx(idx):\n",
    "    plt.cla()\n",
    "    c_df = average_field_df[average_field_df['frame_idx'].isin([idx])]\n",
    "    dx_func = img_intp(interp2d(c_df['x'], c_df['y'], c_df['dx']))\n",
    "    dy_func = img_intp(interp2d(c_df['x'], c_df['y'], c_df['dy']))\n",
    "    sns.heatmap(g_x, ax=ax1, annot=False, cbar=False)\n",
    "    ax1.set_title('Frame %d\\nX Position' % idx)\n",
    "    sns.heatmap(g_y, ax=ax2, annot=False, cbar=False)\n",
    "    ax2.set_title('Y Position')\n",
    "\n",
    "    sns.heatmap(dx_func(g_x, g_y), ax=ax3, annot=False, cbar=False, fmt='2.1f')\n",
    "    ax3.set_title('$\\Delta x$')\n",
    "    sns.heatmap(dy_func(g_x, g_y), ax=ax4, annot=False, cbar=False, fmt='2.1f')\n",
    "    ax4.set_title('$\\Delta y$')\n",
    "    ax5.quiver(g_x, g_y, dx_func(g_x, g_y), dy_func(g_x, g_y),\n",
    "               scale=1.0, scale_units='xy', angles='xy')\n",
    "\n",
    "\n",
    "# write animation frames\n",
    "anim_code = FuncAnimation(fig,\n",
    "                          draw_frame_idx,\n",
    "                          frames=frames,\n",
    "                          interval=1000,\n",
    "                          repeat_delay=2000).to_html5_video()\n",
    "plt.close('all')\n",
    "HTML(anim_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Longer Series\n",
    "We see that this approach becomes problematic when we want to work with longer series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from skimage.morphology import label\n",
    "from skimage.measure import regionprops\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "fig, c_ax = plt.subplots(1, 1, figsize=(5, 5), dpi=150)\n",
    "\n",
    "s_img = disk_img.copy()\n",
    "img_list = [s_img]\n",
    "for i in range(8):\n",
    "    if i % 2 == 0:\n",
    "        s_img = np.roll(s_img, -2, axis=0)\n",
    "    else:\n",
    "        s_img = np.roll(s_img, -1, axis=1)\n",
    "    img_list += [s_img]\n",
    "\n",
    "all_objs = []\n",
    "for frame_idx, c_img in enumerate(img_list):\n",
    "    lab_img = label(c_img > 0)\n",
    "    for c_obj in regionprops(lab_img):\n",
    "        all_objs += [dict(label=int(c_obj.label),\n",
    "                          y=c_obj.centroid[0],\n",
    "                          x=c_obj.centroid[1],\n",
    "                          area=c_obj.area,\n",
    "                          frame_idx=frame_idx)]\n",
    "all_obj_df = pd.DataFrame(all_objs)\n",
    "all_obj_df.head(5)\n",
    "\n",
    "\n",
    "def update_frame(i):\n",
    "    plt.cla()\n",
    "    sns.heatmap(img_list[i],\n",
    "                annot=True,\n",
    "                fmt=\"d\",\n",
    "                cmap='nipy_spectral',\n",
    "                ax=c_ax,\n",
    "                cbar=False,\n",
    "                vmin=0,\n",
    "                vmax=1)\n",
    "    c_ax.set_title('Iteration #{}'.format(i+1))\n",
    "\n",
    "\n",
    "# write animation frames\n",
    "anim_code = FuncAnimation(fig,\n",
    "                          update_frame,\n",
    "                          frames=len(img_list),\n",
    "                          interval=1000,\n",
    "                          repeat_delay=2000).to_html5_video()\n",
    "plt.close('all')\n",
    "HTML(anim_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig, c_ax = plt.subplots(1, 1, figsize=(5, 5), dpi=150)\n",
    "c_ax.matshow(disk_img > 1,cmap='gist_yarg')\n",
    "\n",
    "\n",
    "def draw_timestep(i):\n",
    "    # plt.cla()\n",
    "    frame_0 = all_obj_df[all_obj_df['frame_idx'].isin([i])]\n",
    "    frame_1 = all_obj_df[all_obj_df['frame_idx'].isin([i+1])]\n",
    "    c_ax.scatter(frame_0['x'], frame_0['y'], c='black', label='Frame: %d' % i)\n",
    "    c_ax.scatter(frame_1['x'], frame_1['y'],\n",
    "                 c='red', label='Frame: %d' % (i+1))\n",
    "    dist_df_list = []\n",
    "    for _, row_0 in frame_0.iterrows():\n",
    "        for _, row_1 in frame_1.iterrows():\n",
    "            dist_df_list += [dict(x0=row_0['x'],\n",
    "                                  y0=row_0['y'],\n",
    "                                  lab0=int(row_0['label']),\n",
    "                                  x1=row_1['x'],\n",
    "                                  y1=row_1['y'],\n",
    "                                  lab1=int(row_1['label']),\n",
    "                                  dist=np.sqrt(\n",
    "                                      np.square(row_0['x']-row_1['x']) +\n",
    "                                      np.square(row_0['y']-row_1['y'])))]\n",
    "    dist_df = pd.DataFrame(dist_df_list)\n",
    "    for _, c_rows in dist_df.groupby('lab0'):\n",
    "        _, best_row = next(c_rows.sort_values('dist').iterrows())\n",
    "        c_ax.quiver(best_row['x0'], best_row['y0'],\n",
    "                    best_row['x1']-best_row['x0'],\n",
    "                    best_row['y1']-best_row['y0'],\n",
    "                    scale=1.0, scale_units='xy', angles='xy', alpha=0.25)\n",
    "    c_ax.set_title('Frame #{}'.format(i+1))\n",
    "\n",
    "\n",
    "# write animation frames\n",
    "anim_code = FuncAnimation(fig,\n",
    "                          draw_timestep,\n",
    "                          frames=all_obj_df['frame_idx'].max(),\n",
    "                          interval=1000,\n",
    "                          repeat_delay=2000).to_html5_video()\n",
    "plt.close('all')\n",
    "HTML(anim_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp2d\n",
    "average_field = []\n",
    "for i in range(all_obj_df['frame_idx'].max()):\n",
    "    frame_0 = all_obj_df[all_obj_df['frame_idx'].isin([i])]\n",
    "    frame_1 = all_obj_df[all_obj_df['frame_idx'].isin([i+1])]\n",
    "    dist_df_list = []\n",
    "    for _, row_0 in frame_0.iterrows():\n",
    "        for _, row_1 in frame_1.iterrows():\n",
    "            dist_df_list += [dict(x0=row_0['x'],\n",
    "                                  y0=row_0['y'],\n",
    "                                  lab0=int(row_0['label']),\n",
    "                                  x1=row_1['x'],\n",
    "                                  y1=row_1['y'],\n",
    "                                  lab1=int(row_1['label']),\n",
    "                                  dist=np.sqrt(\n",
    "                                      np.square(row_0['x']-row_1['x']) +\n",
    "                                      np.square(row_0['y']-row_1['y'])))]\n",
    "    dist_df = pd.DataFrame(dist_df_list)\n",
    "    for _, c_rows in dist_df.groupby('lab0'):\n",
    "        _, best_row = next(c_rows.sort_values('dist').iterrows())\n",
    "        average_field += [dict(frame_idx=i,\n",
    "                               x=best_row['x0'],\n",
    "                               y=best_row['y0'],\n",
    "                               dx=best_row['x1']-best_row['x0'],\n",
    "                               dy=best_row['y1']-best_row['y0'])]\n",
    "average_field_df = pd.DataFrame(average_field)\n",
    "print('Average Flow:')\n",
    "print(average_field_df[['dx', 'dy']].mean())\n",
    "\n",
    "\n",
    "def img_intp(f):\n",
    "    def new_f(x, y):\n",
    "        return np.stack([f(ix, iy) for ix, iy in zip(np.ravel(x), np.ravel(y))], 0).reshape(np.shape(x))\n",
    "    return new_f\n",
    "\n",
    "\n",
    "dx_func = img_intp(\n",
    "    interp2d(average_field_df['x'], average_field_df['y'], average_field_df['dx']))\n",
    "dy_func = img_intp(\n",
    "    interp2d(average_field_df['x'], average_field_df['y'], average_field_df['dy']))\n",
    "\n",
    "g_x, g_y = np.meshgrid(np.linspace(average_field_df['x'].min(),\n",
    "                                   average_field_df['x'].max(), 5),\n",
    "                       np.linspace(average_field_df['y'].min(),\n",
    "                                   average_field_df['y'].max(), 5))\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(24, 4))\n",
    "sns.heatmap(g_x, ax=ax1, annot=True)\n",
    "ax1.set_title('X Position')\n",
    "sns.heatmap(g_y, ax=ax2, annot=True)\n",
    "ax2.set_title('Y Position')\n",
    "\n",
    "sns.heatmap(dx_func(g_x, g_y), ax=ax3, annot=True)\n",
    "ax3.set_title('$\\Delta x$')\n",
    "sns.heatmap(dy_func(g_x, g_y), ax=ax4, annot=True)\n",
    "ax4.set_title('$\\Delta y$')\n",
    "ax5.quiver(g_x, g_y, dx_func(g_x, g_y), dy_func(g_x, g_y),\n",
    "           scale=1.0, scale_units='xy', angles='xy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "temp_avg_field = average_field_df[['frame_idx', 'dx', 'dy']].groupby(\n",
    "    'frame_idx').agg('mean').reset_index()\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 4))\n",
    "ax1.plot(temp_avg_field['frame_idx'], temp_avg_field['dx'], 'rs-')\n",
    "ax1.set_title('$\\Delta x$')\n",
    "ax1.set_xlabel('Timestep')\n",
    "ax2.plot(temp_avg_field['frame_idx'], temp_avg_field['dy'], 'rs-')\n",
    "ax2.set_title('$\\Delta y$')\n",
    "ax2.set_xlabel('Timestep')\n",
    "ax3.quiver(temp_avg_field['dx'], temp_avg_field['dy'],\n",
    "           scale=1, scale_units='xy', angles='xy')\n",
    "ax3.set_title('$\\Delta x$, $\\Delta y$')\n",
    "ax3.set_xlabel('Timestep')\n",
    "temp_avg_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "g_x, g_y = np.meshgrid(np.linspace(average_field_df['x'].min(),\n",
    "                                   average_field_df['x'].max(), 4),\n",
    "                       np.linspace(average_field_df['y'].min(),\n",
    "                                   average_field_df['y'].max(), 4))\n",
    "\n",
    "frames = len(sorted(np.unique(average_field_df['frame_idx'])))\n",
    "fig, m_axs = plt.subplots(2, 3, figsize=(14, 10))\n",
    "for c_ax in m_axs.flatten():\n",
    "    c_ax.axis('off')\n",
    "[(ax1, ax2, _), (ax3, ax4, ax5)] = m_axs\n",
    "\n",
    "\n",
    "def draw_frame_idx(idx):\n",
    "    plt.cla()\n",
    "    c_df = average_field_df[average_field_df['frame_idx'].isin([idx])]\n",
    "    dx_func = img_intp(interp2d(c_df['x'], c_df['y'], c_df['dx']))\n",
    "    dy_func = img_intp(interp2d(c_df['x'], c_df['y'], c_df['dy']))\n",
    "    sns.heatmap(g_x, ax=ax1, annot=False, cbar=False)\n",
    "    ax1.set_title('Frame %d\\nX Position' % idx)\n",
    "    sns.heatmap(g_y, ax=ax2, annot=False, cbar=False)\n",
    "    ax2.set_title('Y Position')\n",
    "\n",
    "    sns.heatmap(dx_func(g_x, g_y), ax=ax3, annot=False, cbar=False, fmt='2.1f')\n",
    "    ax3.set_title('$\\Delta x$')\n",
    "    sns.heatmap(dy_func(g_x, g_y), ax=ax4, annot=False, cbar=False, fmt='2.1f')\n",
    "    ax4.set_title('$\\Delta y$')\n",
    "    ax5.quiver(g_x, g_y, dx_func(g_x, g_y), dy_func(g_x, g_y),\n",
    "               scale=1.0, scale_units='xy', angles='xy')\n",
    "\n",
    "\n",
    "# write animation frames\n",
    "anim_code = FuncAnimation(fig,\n",
    "                          draw_frame_idx,\n",
    "                          frames=frames,\n",
    "                          interval=1000,\n",
    "                          repeat_delay=2000).to_html5_video()\n",
    "plt.close('all')\n",
    "HTML(anim_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Appearance / Disappearance\n",
    "\n",
    "\n",
    "Under perfect imaging and experimental conditions objects should not appear and reappear but due to \n",
    "- Noise\n",
    "- Limited fields of view / depth of field\n",
    "- Discrete segmentation approachs\n",
    "- Motion artifacts\n",
    "    - Blurred objects often have lower intensity values than still objects\n",
    "\n",
    "It is common for objects to appear and vanish regularly in an experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Jitter / Motion Noise\n",
    "\n",
    "Even perfect spherical objects do not move in a straight line: \n",
    "\n",
    "- The jitter can be seen as a stochastic variable with a random magnitude ($a$) and angle ($b$). \n",
    "- This is then sampled at every point in the field\n",
    "\n",
    "$$ \\vec{v}(\\vec{x})=\\vec{v}_L(\\vec{x})+||a||\\measuredangle b $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Limits of Tracking\n",
    "\n",
    "We see that visually tracking samples can be difficult and there are a number of parameters which affect the ability for us to clearly see the tracking.\n",
    "- flow rate\n",
    "- flow type\n",
    "- density\n",
    "- appearance and disappearance rate \n",
    "- jitter\n",
    "- particle uniqueness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to improve tracking\n",
    "We have to try to quantify the limits of these parameters for different tracking methods in order to design experiments better. \n",
    "\n",
    "### Acquisition-based Parameters\n",
    "\n",
    "- Acquisition rate \n",
    "    - flow rate vs. sampling rate \n",
    "    - jitter (per frame)\n",
    "- Resolution  \n",
    "    - density, \n",
    "    - appearance rate\n",
    "\n",
    "\n",
    "### Experimental Parameters\n",
    "\n",
    "- Experimental setup (pressure, etc) $\\rightarrow$ flow rate/type \n",
    "- Polydispersity $\\rightarrow$ particle uniqueness\n",
    "- Vibration/temperature $\\rightarrow$ jitter\n",
    "- Mixture $\\rightarrow$ density contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Basic Simulations\n",
    "\n",
    "\n",
    "Input flow from simulation\n",
    "\n",
    "$$ \\vec{v}(\\vec{x})=\\langle 0,0,0.05 \\rangle+||0.01||\\measuredangle b $$\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"../common/figures/basic_flow.png\" style=\"width:300px\" />\n",
    "</td>\n",
    "<td><img src=\"../common/figures/basic_flow_tracked.png\" style=\"width:300px\" /></td></tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Designing Experiments\n",
    "\n",
    "\n",
    "### How does this help us to design experiments?\n",
    "- density can be changed by adjusting the concentration of the substances being examined or the field of view\n",
    "- flow per frame (image velocity) can usually be adjusted by changing pressure or acquisition time\n",
    "- jitter can be estimated from images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How much is enough?\n",
    "#### Difficult to create one number for every experiment\n",
    "#### 5% error in bubble position $\\rightarrow$\n",
    "- <5% in flow field\n",
    "- \\>20% error in topology\n",
    "    \n",
    "#### 5% error in shape or volume $\\rightarrow$ \n",
    "- 5% in distribution or changes  \n",
    "- \\> 5% in individual bubble changes\n",
    "- \\> 15% for single bubble strain tensor calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extending Nearest Neighbor\n",
    "\n",
    "\n",
    "### Bijective Requirement\n",
    "\n",
    "We define $\\vec{P}_f$ as the result of performing the nearest neigbhor tracking on $\\vec{P}_0$\n",
    "$$ \\vec{P}_f=\\textrm{argmin}(||\\vec{P}_0-\\vec{y} || \\forall \\vec{y}\\in I_1) $$\n",
    "\n",
    "We define $\\vec{P}_i$ as the result of performing the nearest neigbhor tracking on $\\vec{P}_f$\n",
    "$$ \\vec{P}_i=\\textrm{argmin}(||\\vec{P}_f-\\vec{y} || \\forall \\vec{y}\\in I_0) $$\n",
    "\n",
    "We say the tracking is bijective if these two points are the same\n",
    "$$ \\vec{P}_i \\stackrel{?}{=} \\vec{P}_0 $$\n",
    "\n",
    "\n",
    "\n",
    "### Maximum Displacement\n",
    "\n",
    "$$ \\vec{P}_1=\\begin{cases} \n",
    "||\\vec{P}_0-\\vec{y} ||<\\textrm{MAXD}, & \\textrm{argmin}(||\\vec{P}_0-\\vec{y} || \\forall \\vec{y}\\in I_1) \\\\\n",
    "\\textrm{Otherwise}, & \\emptyset \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Extending Nearest Neighbor (Continued)\n",
    "\n",
    "Models of movement behavior to support tracking\n",
    "\n",
    "### Prior / Expected Movement\n",
    "\n",
    "$$\\vec{P}_1=\\textrm{argmin}(||\\vec{P}_0+\\vec{v}_{offset}-\\vec{y} || \\forall \\vec{y}\\in I_1)$$\n",
    "\n",
    "### Adaptive Movement\n",
    "Can then be calculated in an iterative fashion where the offset is the average from all of the $\\vec{P}_1-\\vec{P}_0$ vectors. It can also be performed \n",
    "$$ \\vec{P}_1=\\textrm{argmin}(||\\vec{P}_0+\\vec{v}_{offset}-\\vec{y} || \\forall \\vec{y}\\in I_1) $$\n",
    "\n",
    "### More advanced models\n",
    "- Use expected physical model\n",
    "- Use track derivative to find $v_{offset}$\n",
    "- Kalman filters can be used for particle tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Beyond Nearest Neighbor\n",
    "\n",
    "\n",
    "While nearest neighbor \n",
    "- provides a useful starting tool \n",
    "- it is not sufficient for truly complicated flows and datasets.\n",
    "\n",
    "### Better Approaches\n",
    "\n",
    "#### Multiple Hypothesis Testing\n",
    "- Nearest neighbor just compares the points between two frames and there is much more information available in most time-resolved datasets. \n",
    "\n",
    "- This approach allows for multiple possible paths to be explored at the same time and the best chosen only after all frames have been examined\n",
    "\n",
    "\n",
    "\n",
    "### Shortcomings\n",
    "\n",
    "#### Merging and Splitting Particles\n",
    "- The simplicity of the nearest neighbor model does really allow for particles to merge and split (relaxing the bijective requirement allows such behavior, but the method is still not suited for such tracking). \n",
    "- For such systems a more specific, physically-based is required to encapsulate this behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Voxel-based Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Voxel-based Approaches\n",
    "\n",
    "For voxel-based approachs the most common analyses are digital image correlation (or for 3D images digital volume correlation), where the correlation is calculated between two images or volumes.\n",
    "\n",
    "### Standard Image Correlation\n",
    "\n",
    "Given images $I_0(\\vec{x})$ and $I_1(\\vec{x})$ at time $t_0$ and $t_1$ respectively. The correlation between these two images can be calculated for each $\\vec{r}$\n",
    "\n",
    "$$ C_{I_0,I_1}(\\vec{r})=\\langle I_0(\\vec{x}) I_1(\\vec{x}+\\vec{r}) \\rangle $$\n",
    "\n",
    "This can also be done in the Fourier space\n",
    "\n",
    "$$C_{I_0,I_1}(\\vec{r})=\\mathcal{F}^{-1}\\{\\mathcal{F}\\{I_0\\}\\cdot \\mathcal{F}\\{I_1\\}^{*}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's make some test data\n",
    "We use a 'five' from the MNIST data set of handwritten numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "%matplotlib inline\n",
    "bw_img = np.load('../common/data/five.npy') # A 'five' from the mnist data set\n",
    "\n",
    "shift_img = np.roll(np.roll(bw_img, -3, axis=0), 2, axis=1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), dpi=100)\n",
    "sns.heatmap(bw_img, ax=ax1, cbar=False, annot=True, fmt='d', cmap='bone'), ax1.set_title('$T_0$')\n",
    "sns.heatmap(shift_img, ax=ax2, cbar=False, annot=True, fmt='d', cmap='bone') , ax2.set_title('$T_1$ ($T_0$ shifted by (-3,2))');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Demonstration of the correlation in space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "mx, my = np.meshgrid(np.arange(-4, 6, 2),\n",
    "                     np.arange(-4, 6, 2))\n",
    "\n",
    "nx = mx.ravel()\n",
    "ny = my.ravel()\n",
    "out_score = np.zeros(nx.shape, dtype=np.float32)\n",
    "\n",
    "\n",
    "def update_frame(i):\n",
    "    a_img = bw_img\n",
    "    b_img = np.roll(np.roll(shift_img, nx[i], axis=1), ny[i], axis=0)\n",
    "    ax1.cla()\n",
    "    sns.heatmap(a_img, ax=ax1, cbar=False, annot=True, fmt='d', cmap='bone')\n",
    "    ax2.cla()\n",
    "    sns.heatmap(b_img, ax=ax2, cbar=False, annot=True, fmt='d', cmap='bone')\n",
    "\n",
    "    out_score[i] = np.mean(a_img*b_img)\n",
    "    ax3.cla()\n",
    "    sns.heatmap(out_score.reshape(mx.shape), ax=ax3,\n",
    "                cbar=False, annot=True, fmt='2.1f', cmap='RdBu')\n",
    "    ax3.set_xticklabels(mx[0, :])\n",
    "    ax3.set_yticklabels(my[:, 0])\n",
    "    ax1.set_title('Iteration #{}'.format(i+1))\n",
    "    ax2.set_title('X-Offset: %d\\nY-Offset: %d' % (nx[i], ny[i]))\n",
    "    ax3.set_title(r'$\\langle I_0(\\vec{x}) I_1(\\vec{x}+\\vec{r}) \\rangle$')\n",
    "\n",
    "\n",
    "# write animation frames\n",
    "anim_code = FuncAnimation(fig,\n",
    "                          update_frame,\n",
    "                          frames=len(nx),\n",
    "                          interval=300,\n",
    "                          repeat_delay=4000).to_html5_video()\n",
    "plt.close('all')\n",
    "HTML(anim_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Other metrics\n",
    "\n",
    "### Mean Squared Error\n",
    "We can also use \n",
    "- MSE \n",
    "- or RMSE \n",
    "\n",
    "and look for minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 6))\n",
    "out_score = np.zeros(nx.shape, dtype=np.float32)\n",
    "for i in range(len(nx)):\n",
    "    a_img = bw_img\n",
    "    b_img = np.roll(np.roll(shift_img, nx[i], axis=1), ny[i], axis=0)\n",
    "    out_score[i] = np.mean(np.square(a_img-b_img))\n",
    "\n",
    "# get the minimum\n",
    "i_min = np.argmin(out_score)\n",
    "b_img = np.roll(np.roll(shift_img, nx[i_min], axis=1), ny[i_min], axis=0)\n",
    "\n",
    "sns.heatmap(a_img, ax=ax1, cbar=False, annot=True, fmt='d', cmap='bone')\n",
    "sns.heatmap(b_img, ax=ax2, cbar=False, annot=True, fmt='d', cmap='bone')\n",
    "sns.heatmap(out_score.reshape(mx.shape), ax=ax3, cbar=False,\n",
    "            annot=True, fmt='2.1f', cmap='viridis')\n",
    "ax3.set_xticklabels(mx[0, :])\n",
    "ax3.set_yticklabels(my[:, 0])\n",
    "ax1.set_title('Iteration #{}'.format(i+1))\n",
    "ax2.set_title('X-Offset: %d\\nY-Offset: %d' % (nx[i_min], ny[i_min]))\n",
    "ax3.set_title(r'$\\langle (I_0(\\vec{x})-I_1(\\vec{x}+\\vec{r}))^2 \\rangle$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Correlation using the Fourier transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(2,3,1); plt.imshow(bw_img); plt.title('Image A')\n",
    "plt.subplot(2,3,4); plt.imshow(shift_img); plt.title('Image B')\n",
    "\n",
    "fa=(np.fft.fft2(bw_img));\n",
    "fb=(np.fft.fft2(shift_img));\n",
    "\n",
    "plt.subplot(2,3,2); plt.imshow(np.abs(np.fft.fftshift(fa))); plt.title('$|\\mathcal{F}(A)|$')\n",
    "plt.subplot(2,3,5); plt.imshow(np.abs(np.fft.fftshift(fb))); plt.title('$|\\mathcal{F}(B)|$')\n",
    "\n",
    "f=fa*np.conjugate(fb);\n",
    "co=np.abs(np.fft.fftshift(np.fft.ifft2(f)));\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(np.abs(co), extent = [-7 , 6, 6 , -7], cmap='viridis');plt.title('Correlation image between a and b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Real Example \n",
    "## Bone Slice Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.filters import median\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "%matplotlib inline\n",
    "full_img = imread(\"ext-figures/bonegfiltslice.png\")\n",
    "full_shift_img = median(\n",
    "    np.roll(np.roll(full_img, -15, axis=0), 15, axis=1), np.ones((1, 3)))\n",
    "\n",
    "def g_roi(x): return x[5:90, 150:275]\n",
    "\n",
    "bw_img = g_roi(full_img)\n",
    "\n",
    "shift_img = g_roi(full_shift_img)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6), dpi=100)\n",
    "ax1.imshow(bw_img, cmap='bone'),ax1.set_title('Image $T_0$')\n",
    "ax2.imshow(shift_img, cmap='bone'),ax2.set_title('Image $T_1$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let's look at a smaller region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6), dpi=100)\n",
    "\n",
    "def g_roi(x): return x[20:30, 210:225]\n",
    "\n",
    "sns.heatmap(g_roi(full_img), ax=ax1, cbar=False,\n",
    "            annot=True, fmt='d', cmap='bone'), ax1.set_title('Image $T_0$')\n",
    "sns.heatmap(g_roi(full_shift_img), ax=ax2, cbar=False,\n",
    "            annot=True, fmt='d', cmap='bone'); ax2.set_title('Image $T_1$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "def g_roi(x): return x[20:30:2, 210:225:2]\n",
    "\n",
    "mx, my = np.meshgrid(np.arange(-10, 12, 4),\n",
    "                     np.arange(-10, 12, 4))\n",
    "\n",
    "nx = mx.ravel()\n",
    "ny = my.ravel()\n",
    "out_score = np.zeros(nx.shape, dtype=np.float32)\n",
    "\n",
    "def update_frame(i):\n",
    "    a_img = g_roi(full_img)\n",
    "    b_img = g_roi(np.roll(np.roll(full_shift_img, nx[i], axis=1), ny[i], axis=0))\n",
    "    ax1.cla(), sns.heatmap(a_img, ax=ax1, cbar=False, annot=True, fmt='d', cmap='bone')\n",
    "    ax2.cla(), sns.heatmap(b_img, ax=ax2, cbar=False, annot=True, fmt='d', cmap='bone')\n",
    "    out_score[i] = np.mean(np.square(a_img-b_img))\n",
    "    ax3.cla(), sns.heatmap(out_score.reshape(mx.shape), ax=ax3, cbar=False, annot=True, fmt='2.1f', cmap='RdBu')\n",
    "    ax1.set_title('Iteration #{}'.format(i+1))\n",
    "    ax2.set_title('X-Offset: %d\\nY-Offset: %d' % (2*nx[i], 2*ny[i]))\n",
    "    ax3.set_xticklabels(mx[0, :])\n",
    "    ax3.set_yticklabels(my[:, 0])\n",
    "\n",
    "# write animation frames\n",
    "anim_code = FuncAnimation(fig,\n",
    "                          update_frame,\n",
    "                          frames=len(nx),\n",
    "                          interval=300,\n",
    "                          repeat_delay=2000).to_html5_video()\n",
    "plt.close('all')\n",
    "HTML(anim_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 6), dpi=100)\n",
    "\n",
    "\n",
    "mx, my = np.meshgrid(np.arange(-20, 25, 5),\n",
    "                     np.arange(-20, 25, 5))\n",
    "\n",
    "nx = mx.ravel()\n",
    "ny = my.ravel()\n",
    "out_score = np.zeros(nx.shape, dtype=np.float32)\n",
    "\n",
    "out_score = np.zeros(nx.shape, dtype=np.float32)\n",
    "\n",
    "\n",
    "def g_roi(x): return x[5:90, 150:275]\n",
    "\n",
    "\n",
    "for i in range(len(nx)):\n",
    "    a_img = g_roi(full_img)\n",
    "    b_img = g_roi(\n",
    "        np.roll(np.roll(full_shift_img, nx[i], axis=1), ny[i], axis=0))\n",
    "    out_score[i] = np.mean(np.square(a_img-b_img))\n",
    "\n",
    "# get the minimum\n",
    "i_min = np.argmin(out_score)\n",
    "b_img = g_roi(np.roll(np.roll(full_shift_img, nx[i_min], axis=1), ny[i_min], axis=0))\n",
    "ax1.imshow(a_img, cmap='bone'), ax1.set_title('$T_0$')\n",
    "ax2.imshow(b_img, cmap='bone'), ax2.set_title('$T_1$ Registered')\n",
    "sns.heatmap(out_score.reshape(mx.shape), ax=ax3, cbar=False,annot=True, fmt='2.1f', cmap='viridis')\n",
    "ax3.set_xticklabels(mx[0, :]), ax3.set_yticklabels(my[:, 0])\n",
    "ax1.set_title('Iteration #{}'.format(i+1))\n",
    "ax2.set_title('X-Offset: %d\\nY-Offset: %d' % (nx[i_min], ny[i_min]))\n",
    "ax3.set_title(r'$\\langle (I_0(\\vec{x})-I_1(\\vec{x}+\\vec{r}))^2 \\rangle$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Registration\n",
    "\n",
    "Before any meaningful tracking tasks can be performed, the first step is to register the measurements so they are all on the same coordinate system. \n",
    "\n",
    "Often the registration can be done along with the tracking by separating the movement into actual sample movement and other (camera, setup, etc) if the motion of either the sample or the other components can be well modeled.\n",
    "\n",
    "In medicine this is frequently needed because different scanners produce different kinds of outputs with different scales, positioning and resolutions. This is also useful for 'follow-up' scans with patients to identify how a disease has progressed. With scans like chest X-rays it isn't uncommon to have multiple (some patients have hundreds) all taken under different conditions\n",
    "\n",
    "![Chest XRays](ext-figures/cxr_nih.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Process\n",
    "We informally followed a process before when trying to match the two images together, but we want to make this more generic for a larger spectrum of problems. \n",
    "\n",
    "We thus follow the model set forward by tools like [ITK](https://itk.org/ITKSoftwareGuide/html/Book2/ITKSoftwareGuide-Book2ch3.html) with the components divided into the input data:\n",
    "- *Moving Image* \n",
    "- and *Fixed Image* sometimes called *Reference Image*).\n",
    "\n",
    "### The algorithmic components\n",
    "- The *Transform* operation to transform the moving image. \n",
    "- The *interpolator* to handle bringing all of the points onto a pixel grid. \n",
    "- The *Metric* which is the measure of how well the transformed moving image and fixed image match \n",
    "- and finally the *Optimizer* that tries to find the best solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from subprocess import check_output\n",
    "import pydot\n",
    "import os\n",
    "\n",
    "\n",
    "def show_graph(graph):\n",
    "    try:\n",
    "        return SVG(graph.create_svg())\n",
    "    except AttributeError as e:\n",
    "        output = check_output('dot -Tsvg', shell=True,\n",
    "                              input=g.to_string().encode())\n",
    "        return SVG(output.decode())\n",
    "\n",
    "\n",
    "g = pydot.Graph(graph_type='digraph')\n",
    "fixed_img = pydot.Node('Fixed Image\\nReference Image',\n",
    "                       shape='folder', style=\"filled\", fillcolor=\"lightgreen\")\n",
    "moving_img = pydot.Node('Moving Image', shape='folder',\n",
    "                        style=\"filled\", fillcolor=\"lightgreen\")\n",
    "trans_obj = pydot.Node('Transform', shape='box',\n",
    "                       style='filled', fillcolor='yellow')\n",
    "g.add_node(fixed_img)\n",
    "g.add_node(moving_img)\n",
    "g.add_node(trans_obj)\n",
    "g.add_edge(pydot.Edge(fixed_img, 'Metric'))\n",
    "g.add_edge(pydot.Edge(moving_img, 'Interpolator'))\n",
    "g.add_edge(pydot.Edge(trans_obj, 'Interpolator', label='Transform Parameters'))\n",
    "g.add_edge(pydot.Edge('Interpolator', 'Metric'))\n",
    "show_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "g.add_edge(pydot.Edge('Metric', 'Optimizer'))\n",
    "g.add_edge(pydot.Edge('Optimizer', trans_obj))\n",
    "show_graph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Images\n",
    "### Fixed Image\n",
    "The fixed image (or reference image) is the image that will be left untouched and used for comparison\n",
    "### Moving Image\n",
    "The moving image will be transformed (translated, scaled, rotated, deformed, ...) to try and match as closely as possible the fixed image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transform\n",
    "The transform specifies the transformations which can take place on the moving image, a number of different types are possible, but the most frequent types are listed below.\n",
    "- Affine\n",
    "- Translation\n",
    "- Scaling\n",
    "- Deformable\n",
    "- Shearing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Interpolator\n",
    "The interpolator is the component applies the transform to the moving image. The common ways of interpolating are\n",
    "- Nearest Neighbor\n",
    "- Bilinear\n",
    "- Bicubic\n",
    "- Bspline\n",
    "- ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Metric\n",
    "\n",
    "The metric is how the success of the matching of the two images is measured. The goal is to measure similarity between images.\n",
    "- Mean Squared Error - the simplist metric to use just recording the raw difference, but often this can lead to unusual matches since noise and uneven illumination can lead to high MSE for images that match well.\n",
    "- SSIM similarity metric\n",
    "- Correlation Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "The optimizer component is responsible for updating the parameters based on the metric. A standard approach with this is gradient descent where the gradient is calculated and a small step (determined by the learning rate) is taken in the direction of maximum descent.\n",
    "- Gradient Descent\n",
    "- Adam \n",
    "- Stochastic Gradient Descent\n",
    "- AdaGrad\n",
    "- AdaDelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Our tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "g = pydot.Graph(graph_type='digraph')\n",
    "fixed_img = pydot.Node('Fixed Image\\nReference Image',\n",
    "                       shape='folder', style=\"filled\", fillcolor=\"lightgreen\")\n",
    "moving_img = pydot.Node('Moving Image', shape='folder',\n",
    "                        style=\"filled\", fillcolor=\"lightgreen\")\n",
    "trans_obj = pydot.Node('Transform', shape='box',\n",
    "                       style='filled', fillcolor='yellow')\n",
    "g.add_node(fixed_img)\n",
    "g.add_node(moving_img)\n",
    "g.add_node(trans_obj)\n",
    "g.add_edge(pydot.Edge(fixed_img, 'Metric\\nMean Squared Error'))\n",
    "g.add_edge(pydot.Edge(moving_img, 'Interpolator\\nNearest Neighbor'))\n",
    "g.add_edge(pydot.Edge(trans_obj, 'Interpolator\\nNearest Neighbor',\n",
    "                      label='Transform Parameters'))\n",
    "g.add_edge(pydot.Edge('Interpolator\\nNearest Neighbor',\n",
    "                      'Metric\\nMean Squared Error'))\n",
    "#g.add_edge(pydot.Edge('Metric\\nMean Squared Error', 'Optimizer\\nGrid Search', style = ''))\n",
    "g.add_edge(pydot.Edge('Optimizer\\nGrid Search', trans_obj))\n",
    "show_graph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Registration of the bone image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.filters import median\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "%matplotlib inline\n",
    "full_img = imread(\"ext-figures/bonegfiltslice.png\")\n",
    "full_shift_img = median(\n",
    "    np.roll(np.roll(full_img, -15, axis=0), 15, axis=1), np.ones((1, 3)))\n",
    "\n",
    "\n",
    "def g_roi(x): return x[5:90, 150:275]\n",
    "\n",
    "\n",
    "bw_img = g_roi(full_img)\n",
    "shift_img = g_roi(full_shift_img)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6), dpi=100)\n",
    "ax1.imshow(bw_img, cmap='bone')\n",
    "ax1.set_title('$T_0$')\n",
    "ax2.imshow(shift_img, cmap='bone')\n",
    "ax2.set_title('$T_1$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%file affine_op.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "Code taken from https://github.com/kevinzakka/spatial-transformer-network/blob/master/transformer.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def affine_transform(input_fmap, theta, out_dims=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Spatial Transformer Network layer implementation as described in [1].\n",
    "    The layer is composed of 3 elements:\n",
    "    - localisation_net: takes the original image as input and outputs \n",
    "      the parameters of the affine transformation that should be applied\n",
    "      to the input image.\n",
    "    - affine_grid_generator: generates a grid of (x,y) coordinates that \n",
    "      correspond to a set of points where the input should be sampled \n",
    "      to produce the transformed output.\n",
    "    - bilinear_sampler: takes as input the original image and the grid\n",
    "      and produces the output image using bilinear interpolation.\n",
    "    Input\n",
    "    -----\n",
    "    - input_fmap: output of the previous layer. Can be input if spatial\n",
    "      transformer layer is at the beginning of architecture. Should be \n",
    "      a tensor of shape (B, H, W, C). \n",
    "    - theta: affine transform tensor of shape (B, 6). Permits cropping, \n",
    "      translation and isotropic scaling. Initialize to identity matrix. \n",
    "      It is the output of the localization network.\n",
    "    Returns\n",
    "    -------\n",
    "    - out_fmap: transformed input feature map. Tensor of size (B, H, W, C).\n",
    "    Notes\n",
    "    -----\n",
    "    [1]: 'Spatial Transformer Networks', Jaderberg et. al,\n",
    "         (https://arxiv.org/abs/1506.02025)\n",
    "    \"\"\"\n",
    "    # grab input dimensions\n",
    "    B = tf.shape(input_fmap)[0]\n",
    "    H = tf.shape(input_fmap)[1]\n",
    "    W = tf.shape(input_fmap)[2]\n",
    "    C = tf.shape(input_fmap)[3]\n",
    "\n",
    "    # reshape theta to (B, 2, 3)\n",
    "    theta = tf.reshape(theta, [B, 2, 3])\n",
    "\n",
    "    # generate grids of same size or upsample/downsample if specified\n",
    "    if out_dims:\n",
    "        out_H = out_dims[0]\n",
    "        out_W = out_dims[1]\n",
    "        batch_grids = affine_grid_generator(out_H, out_W, theta)\n",
    "    else:\n",
    "        batch_grids = affine_grid_generator(H, W, theta)\n",
    "\n",
    "    x_s = batch_grids[:, 0, :, :]\n",
    "    y_s = batch_grids[:, 1, :, :]\n",
    "\n",
    "    # sample input with grid to get output\n",
    "    out_fmap = bilinear_sampler(input_fmap, x_s, y_s)\n",
    "\n",
    "    return out_fmap\n",
    "\n",
    "\n",
    "def get_pixel_value(img, x, y):\n",
    "    \"\"\"\n",
    "    Utility function to get pixel value for coordinate\n",
    "    vectors x and y from a  4D tensor image.\n",
    "    Input\n",
    "    -----\n",
    "    - img: tensor of shape (B, H, W, C)\n",
    "    - x: flattened tensor of shape (B*H*W, )\n",
    "    - y: flattened tensor of shape (B*H*W, )\n",
    "    Returns\n",
    "    -------\n",
    "    - output: tensor of shape (B, H, W, C)\n",
    "    \"\"\"\n",
    "    shape = tf.shape(x)\n",
    "    batch_size = shape[0]\n",
    "    height = shape[1]\n",
    "    width = shape[2]\n",
    "\n",
    "    batch_idx = tf.range(0, batch_size)\n",
    "    batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\n",
    "    b = tf.tile(batch_idx, (1, height, width))\n",
    "\n",
    "    indices = tf.stack([b, y, x], 3)\n",
    "\n",
    "    return tf.gather_nd(img, indices)\n",
    "\n",
    "\n",
    "def affine_grid_generator(height, width, theta):\n",
    "    \"\"\"\n",
    "    This function returns a sampling grid, which when\n",
    "    used with the bilinear sampler on the input feature \n",
    "    map, will create an output feature map that is an \n",
    "    affine transformation [1] of the input feature map.\n",
    "    Input\n",
    "    -----\n",
    "    - height: desired height of grid/output. Used\n",
    "      to downsample or upsample. \n",
    "    - width: desired width of grid/output. Used\n",
    "      to downsample or upsample. \n",
    "    - theta: affine transform matrices of shape (num_batch, 2, 3). \n",
    "      For each image in the batch, we have 6 theta parameters of \n",
    "      the form (2x3) that define the affine transformation T.\n",
    "    Returns\n",
    "    -------\n",
    "    - normalized gird (-1, 1) of shape (num_batch, 2, H, W).\n",
    "      The 2nd dimension has 2 components: (x, y) which are the \n",
    "      sampling points of the original image for each point in the\n",
    "      target image.\n",
    "    Note\n",
    "    ----\n",
    "    [1]: the affine transformation allows cropping, translation, \n",
    "         and isotropic scaling.\n",
    "    \"\"\"\n",
    "    # grab batch size\n",
    "    num_batch = tf.shape(theta)[0]\n",
    "\n",
    "    # create normalized 2D grid\n",
    "    x = tf.linspace(-1.0, 1.0, width)\n",
    "    y = tf.linspace(-1.0, 1.0, height)\n",
    "    x_t, y_t = tf.meshgrid(x, y)\n",
    "\n",
    "    # flatten\n",
    "    x_t_flat = tf.reshape(x_t, [-1])\n",
    "    y_t_flat = tf.reshape(y_t, [-1])\n",
    "\n",
    "    # reshape to [x_t, y_t , 1] - (homogeneous form)\n",
    "    ones = tf.ones_like(x_t_flat)\n",
    "    sampling_grid = tf.stack([x_t_flat, y_t_flat, ones])\n",
    "\n",
    "    # repeat grid num_batch times\n",
    "    sampling_grid = tf.expand_dims(sampling_grid, axis=0)\n",
    "    sampling_grid = tf.tile(sampling_grid, tf.stack([num_batch, 1, 1]))\n",
    "\n",
    "    # cast to float32 (required for matmul)\n",
    "    theta = tf.cast(theta, 'float32')\n",
    "    sampling_grid = tf.cast(sampling_grid, 'float32')\n",
    "\n",
    "    # transform the sampling grid - batch multiply\n",
    "    batch_grids = tf.matmul(theta, sampling_grid)\n",
    "    # batch grid has shape (num_batch, 2, H*W)\n",
    "\n",
    "    # reshape to (num_batch, H, W, 2)\n",
    "    batch_grids = tf.reshape(batch_grids, [num_batch, 2, height, width])\n",
    "\n",
    "    return batch_grids\n",
    "\n",
    "\n",
    "def bilinear_sampler(img, x, y):\n",
    "    \"\"\"\n",
    "    Performs bilinear sampling of the input images according to the \n",
    "    normalized coordinates provided by the sampling grid. Note that \n",
    "    the sampling is done identically for each channel of the input.\n",
    "    To test if the function works properly, output image should be\n",
    "    identical to input image when theta is initialized to identity\n",
    "    transform.\n",
    "    Input\n",
    "    -----\n",
    "    - img: batch of images in (B, H, W, C) layout.\n",
    "    - grid: x, y which is the output of affine_grid_generator.\n",
    "    Returns\n",
    "    -------\n",
    "    - interpolated images according to grids. Same size as grid.\n",
    "    \"\"\"\n",
    "    # prepare useful params\n",
    "    B = tf.shape(img)[0]\n",
    "    H = tf.shape(img)[1]\n",
    "    W = tf.shape(img)[2]\n",
    "    C = tf.shape(img)[3]\n",
    "\n",
    "    max_y = tf.cast(H - 1, 'int32')\n",
    "    max_x = tf.cast(W - 1, 'int32')\n",
    "    zero = tf.zeros([], dtype='int32')\n",
    "\n",
    "    # cast indices as float32 (for rescaling)\n",
    "    x = tf.cast(x, 'float32')\n",
    "    y = tf.cast(y, 'float32')\n",
    "\n",
    "    # rescale x and y to [0, W/H]\n",
    "    x = 0.5 * ((x + 1.0) * tf.cast(W, 'float32'))\n",
    "    y = 0.5 * ((y + 1.0) * tf.cast(H, 'float32'))\n",
    "\n",
    "    # grab 4 nearest corner points for each (x_i, y_i)\n",
    "    # i.e. we need a rectangle around the point of interest\n",
    "    x0 = tf.cast(tf.floor(x), 'int32')\n",
    "    x1 = x0 + 1\n",
    "    y0 = tf.cast(tf.floor(y), 'int32')\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    # clip to range [0, H/W] to not violate img boundaries\n",
    "    x0 = tf.clip_by_value(x0, zero, max_x)\n",
    "    x1 = tf.clip_by_value(x1, zero, max_x)\n",
    "    y0 = tf.clip_by_value(y0, zero, max_y)\n",
    "    y1 = tf.clip_by_value(y1, zero, max_y)\n",
    "\n",
    "    # get pixel value at corner coords\n",
    "    Ia = get_pixel_value(img, x0, y0)\n",
    "    Ib = get_pixel_value(img, x0, y1)\n",
    "    Ic = get_pixel_value(img, x1, y0)\n",
    "    Id = get_pixel_value(img, x1, y1)\n",
    "\n",
    "    # recast as float for delta calculation\n",
    "    x0 = tf.cast(x0, 'float32')\n",
    "    x1 = tf.cast(x1, 'float32')\n",
    "    y0 = tf.cast(y0, 'float32')\n",
    "    y1 = tf.cast(y1, 'float32')\n",
    "\n",
    "    # calculate deltas\n",
    "    wa = (x1-x) * (y1-y)\n",
    "    wb = (x1-x) * (y-y0)\n",
    "    wc = (x-x0) * (y1-y)\n",
    "    wd = (x-x0) * (y-y0)\n",
    "\n",
    "    # add dimension for addition\n",
    "    wa = tf.expand_dims(wa, axis=3)\n",
    "    wb = tf.expand_dims(wb, axis=3)\n",
    "    wc = tf.expand_dims(wc, axis=3)\n",
    "    wd = tf.expand_dims(wd, axis=3)\n",
    "\n",
    "    # compute output\n",
    "    out = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from affine_op import affine_transform\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    init = tf.global_variables_initializer()\n",
    "    # tf Graph Input\n",
    "    fixed_img = tf.placeholder(\"float\", shape=(\n",
    "        1, None, None, 1), name='FixedImage')\n",
    "    moving_img = tf.placeholder(\"float\", shape=(\n",
    "        1, None, None, 1), name='MovingImage')\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "\n",
    "    with tf.name_scope('transform_parameters'):  # Set transform parameters\n",
    "        x_offset = tf.Variable(0.0, name=\"x_offset\")\n",
    "        y_offset = tf.Variable(0.0, name=\"y_offset\")\n",
    "        # we keep scale and rotation fixed\n",
    "        scale = tf.placeholder(\"float\", shape=tuple(), name=\"scale\")\n",
    "        rotation = tf.placeholder(\"float\", shape=tuple(), name=\"rotation\")\n",
    "\n",
    "    with tf.name_scope('transformer_and_interpolator'):\n",
    "        flat_mat = tf.tile([tf.cos(rotation), -tf.sin(rotation), x_offset,\n",
    "                            tf.sin(rotation), tf.cos(rotation), y_offset], (1,))\n",
    "        flat_mat = tf.reshape(flat_mat, (1, 6))\n",
    "        trans_tensor = affine_transform(moving_img, flat_mat)\n",
    "\n",
    "    with tf.name_scope('metric'):\n",
    "        mse = tf.reduce_mean(\n",
    "            tf.square(fixed_img-trans_tensor), name='MeanSquareError')\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add()\n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\" % size\n",
    "    return strip_def\n",
    "\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script src=\"//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js\"></script>\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "\n",
    "\n",
    "show_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_feed_dict(f_img, m_img):\n",
    "    return {fixed_img: np.expand_dims(np.expand_dims(f_img, 0), -1),\n",
    "            moving_img: np.expand_dims(np.expand_dims(m_img, 0), -1),\n",
    "            rotation: 0.0}\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "optimize_iters = 10\n",
    "with tf.Session(graph=g) as sess:\n",
    "    plt.close('all')\n",
    "    fig, m_axs = plt.subplots(2, 2, figsize=(10, 10), dpi=100)\n",
    "    #tf.initialize_all_variables().run()\n",
    "    init = tf.global_variables_initializer()\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    # Fit all training data\n",
    "    const_feed_dict = make_feed_dict(bw_img, shift_img)\n",
    "\n",
    "    def update_frame(i):\n",
    "        global loss_history\n",
    "        (ax1, ax2), (ax4, ax3) = m_axs\n",
    "        for c_ax in m_axs.flatten():\n",
    "            c_ax.cla()\n",
    "            c_ax.axis('off')\n",
    "        f_mse, x_pos, y_pos, rs_img = sess.run([mse, x_offset, y_offset, trans_tensor],\n",
    "                                               feed_dict=const_feed_dict)\n",
    "        loss_history += [f_mse]\n",
    "\n",
    "        ax1.imshow(bw_img, cmap='bone')\n",
    "        ax1.set_title('$T_0$')\n",
    "        ax2.imshow(shift_img, cmap='bone')\n",
    "        ax2.set_title('$T_1$')\n",
    "        #ax3.imshow(rs_img[0,:,:,0], cmap = 'bone')\n",
    "        # ax3.set_title('Output')\n",
    "        ax4.imshow(bw_img*1.0-rs_img[0, :, :, 0],\n",
    "                   cmap='RdBu', vmin=-100, vmax=100)\n",
    "        ax4.set_title('Difference\\nMSE: %2.2f' % (f_mse))\n",
    "        ax3.semilogy(loss_history)\n",
    "        ax3.set_xlabel('Iteration')\n",
    "        ax3.set_ylabel('MSE (Log-scale)')\n",
    "        ax3.axis('on')\n",
    "\n",
    "        for _ in range(1):\n",
    "            sess.run(optimizer, feed_dict=const_feed_dict)\n",
    "    # write animation frames\n",
    "    anim_code = FuncAnimation(fig,\n",
    "                              update_frame,\n",
    "                              frames=optimize_iters,\n",
    "                              interval=1000,\n",
    "                              repeat_delay=2000).to_html5_video()\n",
    "    plt.close('all')\n",
    "HTML(anim_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "g_roi = tf.Graph()\n",
    "with g_roi.as_default():\n",
    "    init = tf.global_variables_initializer()\n",
    "    # tf Graph Input\n",
    "    fixed_img = tf.placeholder(\"float\", shape=(\n",
    "        1, None, None, 1), name='FixedImage')\n",
    "    moving_img = tf.placeholder(\"float\", shape=(\n",
    "        1, None, None, 1), name='MovingImage')\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "\n",
    "    with tf.name_scope('transform_parameters'):  # Set transform parameters\n",
    "        x_offset = tf.Variable(0.0, name=\"x_offset\")\n",
    "        y_offset = tf.Variable(0.0, name=\"y_offset\")\n",
    "        # we keep rotation fixed\n",
    "        rotation = tf.placeholder(\"float\", shape=tuple(), name=\"rotation\")\n",
    "\n",
    "    with tf.name_scope('transformer_and_interpolator'):\n",
    "        flat_mat = tf.tile([tf.cos(rotation), -tf.sin(rotation), x_offset,\n",
    "                            tf.sin(rotation), tf.cos(rotation), y_offset], (1,))\n",
    "        flat_mat = tf.reshape(flat_mat, (1, 6))\n",
    "        trans_tensor = affine_transform(moving_img, flat_mat)\n",
    "\n",
    "    with tf.name_scope('metric'):\n",
    "        diff_tensor = (fixed_img-trans_tensor)[:, 25:75, 25:110, :]\n",
    "        mse = tf.reduce_mean(tf.square(diff_tensor), name='MeanSquareError')\n",
    "        optimizer = tf.train.GradientDescentOptimizer(2e-6).minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from matplotlib import patches\n",
    "optimize_iters = 20\n",
    "loss_history = []\n",
    "with tf.Session(graph=g_roi) as sess:\n",
    "    plt.close('all')\n",
    "    fig, m_axs = plt.subplots(2, 3, figsize=(9, 4), dpi=100)\n",
    "    init = tf.global_variables_initializer()\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    # Fit all training data\n",
    "    const_feed_dict = make_feed_dict(bw_img, shift_img)\n",
    "\n",
    "    def update_frame(i):\n",
    "        global loss_history\n",
    "        (ax1, ax2, ax5), (ax3, ax4, ax6) = m_axs\n",
    "        for c_ax in m_axs.flatten():\n",
    "            c_ax.cla()\n",
    "            c_ax.axis('off')\n",
    "        f_mse, x_pos, y_pos, rs_img, diff_img = sess.run([mse, x_offset, y_offset, trans_tensor, diff_tensor],\n",
    "                                                         feed_dict=const_feed_dict)\n",
    "        loss_history += [f_mse]\n",
    "\n",
    "        ax1.imshow(bw_img, cmap='bone')\n",
    "        ax1.set_title('$T_0$')\n",
    "        ax2.imshow(shift_img, cmap='bone')\n",
    "        ax2.set_title('$T_1$')\n",
    "        ax3.imshow(rs_img[0, :, :, 0], cmap='bone')\n",
    "        ax3.set_title('Output')\n",
    "        ax4.imshow(bw_img*1.0-rs_img[0, :, :, 0],\n",
    "                   cmap='RdBu', vmin=-100, vmax=100)\n",
    "        ax4.set_title('MSE: %2.2f' % (f_mse))\n",
    "        rect = patches.Rectangle(\n",
    "            (25, 25), 85, 50, linewidth=2, edgecolor='g', facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax4.add_patch(rect)\n",
    "        ax5.semilogy(loss_history)\n",
    "        ax5.set_xlabel('Iteration')\n",
    "        ax5.set_ylabel('MSE (Log-scale)')\n",
    "        ax5.axis('on')\n",
    "\n",
    "        ax6.imshow(diff_img[0, :, :, 0], cmap='RdBu', vmin=-100, vmax=100)\n",
    "        ax6.set_title('ROI')\n",
    "        for _ in range(5):\n",
    "            sess.run(optimizer, feed_dict=const_feed_dict)\n",
    "    # write animation frames\n",
    "    anim_code = FuncAnimation(fig,\n",
    "                              update_frame,\n",
    "                              frames=optimize_iters,\n",
    "                              interval=1000,\n",
    "                              repeat_delay=2000).to_html5_video()\n",
    "    plt.close('all')\n",
    "HTML(anim_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Smoother Gradient\n",
    "We can use a distance map of the segmentation to give us a smoother gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.filters import threshold_otsu\n",
    "fig, [(ax1, ax2), (ax3, ax4)] = plt.subplots(2, 2, figsize=(8, 8))\n",
    "thresh_img = bw_img > threshold_otsu(bw_img)\n",
    "dist_start_img = distance_transform_edt(thresh_img)\n",
    "dist_shift_img = distance_transform_edt(shift_img > threshold_otsu(bw_img))\n",
    "\n",
    "ax1.imshow(bw_img, cmap='bone')\n",
    "ax2.imshow(thresh_img, cmap='bone')\n",
    "ax3.imshow(dist_start_img, cmap='jet')\n",
    "ax3.set_title('dmap Fixed Image')\n",
    "ax4.imshow(dist_shift_img, cmap='jet')\n",
    "ax4.set_title('dmap Moving Image');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from matplotlib import patches\n",
    "optimize_iters = 20\n",
    "loss_history = []\n",
    "with tf.Session(graph=g_roi) as sess:\n",
    "    plt.close('all')\n",
    "    fig, m_axs = plt.subplots(2, 3, figsize=(12, 4), dpi=100)\n",
    "    # Run the initializer\n",
    "    tf.initialize_all_variables().run()\n",
    "    # Fit all training data\n",
    "    const_feed_dict = make_feed_dict(dist_start_img, dist_shift_img)\n",
    "    real_image_feed_dict = make_feed_dict(bw_img, shift_img)\n",
    "\n",
    "    def update_frame(i):\n",
    "        global loss_history\n",
    "        (ax1, ax2, ax5), (ax3, ax4, ax6) = m_axs\n",
    "        for c_ax in m_axs.flatten():\n",
    "            c_ax.cla()\n",
    "            c_ax.axis('off')\n",
    "        f_mse, x_pos, y_pos, rs_img, diff_img = sess.run([mse, x_offset, y_offset, trans_tensor, diff_tensor],\n",
    "                                                         feed_dict=const_feed_dict)\n",
    "        real_rs_img, real_diff_img = sess.run([trans_tensor, diff_tensor],\n",
    "                                              feed_dict=real_image_feed_dict)\n",
    "\n",
    "        loss_history += [f_mse]\n",
    "\n",
    "        ax1.imshow(bw_img, cmap='bone')\n",
    "        ax1.set_title('$T_0$')\n",
    "        ax2.imshow(shift_img, cmap='bone')\n",
    "        ax2.set_title('$T_1$')\n",
    "        ax3.imshow(real_rs_img[0, :, :, 0], cmap='bone')\n",
    "        ax3.set_title('Output')\n",
    "        ax4.imshow(dist_start_img*1.0 -\n",
    "                   rs_img[0, :, :, 0], cmap='RdBu', vmin=-10, vmax=10)\n",
    "        ax4.set_title('MSE: %2.2f' % (f_mse))\n",
    "        rect = patches.Rectangle(\n",
    "            (25, 25), 75, 50, linewidth=2, edgecolor='g', facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax4.add_patch(rect)\n",
    "        ax5.semilogy(loss_history)\n",
    "        ax5.set_xlabel('Iteration')\n",
    "        ax5.set_ylabel('MSE\\n(Log-scale)')\n",
    "        ax5.axis('on')\n",
    "\n",
    "        ax6.imshow(diff_img[0, :, :, 0], cmap='RdBu', vmin=-10, vmax=10)\n",
    "        ax6.set_title('ROI')\n",
    "        for _ in range(200):\n",
    "            sess.run(optimizer, feed_dict=const_feed_dict)\n",
    "    # write animation frames\n",
    "    anim_code = FuncAnimation(fig,\n",
    "                              update_frame,\n",
    "                              frames=optimize_iters,\n",
    "                              interval=1000,\n",
    "                              repeat_delay=2000).to_html5_video()\n",
    "    plt.close('all')\n",
    "HTML(anim_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%file backport_ssim.py\n",
    "from tensorflow.python.ops import array_ops, control_flow_ops, check_ops, math_ops, nn_ops, nn\n",
    "from tensorflow.python.framework import constant_op, dtypes, ops\n",
    "\n",
    "# backporting new tensorflow ops is soo much fun\n",
    "_SSIM_K1 = 0.01\n",
    "_SSIM_K2 = 0.03\n",
    "\n",
    "\n",
    "def _ssim_helper(x, y, reducer, max_val, compensation=1.0):\n",
    "    r\"\"\"Helper function for computing SSIM.\n",
    "    SSIM estimates covariances with weighted sums.  The default parameters\n",
    "    use a biased estimate of the covariance:\n",
    "    Suppose `reducer` is a weighted sum, then the mean estimators are\n",
    "      \\mu_x = \\sum_i w_i x_i,\n",
    "      \\mu_y = \\sum_i w_i y_i,\n",
    "    where w_i's are the weighted-sum weights, and covariance estimator is\n",
    "      cov_{xy} = \\sum_i w_i (x_i - \\mu_x) (y_i - \\mu_y)\n",
    "    with assumption \\sum_i w_i = 1. This covariance estimator is biased, since\n",
    "      E[cov_{xy}] = (1 - \\sum_i w_i ^ 2) Cov(X, Y).\n",
    "    For SSIM measure with unbiased covariance estimators, pass as `compensation`\n",
    "    argument (1 - \\sum_i w_i ^ 2).\n",
    "    Arguments:\n",
    "      x: First set of images.\n",
    "      y: Second set of images.\n",
    "      reducer: Function that computes 'local' averages from set of images.\n",
    "        For non-covolutional version, this is usually tf.reduce_mean(x, [1, 2]),\n",
    "        and for convolutional version, this is usually tf.nn.avg_pool or\n",
    "        tf.nn.conv2d with weighted-sum kernel.\n",
    "      max_val: The dynamic range (i.e., the difference between the maximum\n",
    "        possible allowed value and the minimum allowed value).\n",
    "      compensation: Compensation factor. See above.\n",
    "    Returns:\n",
    "      A pair containing the luminance measure, and the contrast-structure measure.\n",
    "    \"\"\"\n",
    "    c1 = (_SSIM_K1 * max_val) ** 2\n",
    "    c2 = (_SSIM_K2 * max_val) ** 2\n",
    "\n",
    "    # SSIM luminance measure is\n",
    "    # (2 * mu_x * mu_y + c1) / (mu_x ** 2 + mu_y ** 2 + c1).\n",
    "    mean0 = reducer(x)\n",
    "    mean1 = reducer(y)\n",
    "    num0 = mean0 * mean1 * 2.0\n",
    "    den0 = math_ops.square(mean0) + math_ops.square(mean1)\n",
    "    luminance = (num0 + c1) / (den0 + c1)\n",
    "\n",
    "    # SSIM contrast-structure measure is\n",
    "    #   (2 * cov_{xy} + c2) / (cov_{xx} + cov_{yy} + c2).\n",
    "    # Note that `reducer` is a weighted sum with weight w_k, \\sum_i w_i = 1, then\n",
    "    #   cov_{xy} = \\sum_i w_i (x_i - \\mu_x) (y_i - \\mu_y)\n",
    "    #          = \\sum_i w_i x_i y_i - (\\sum_i w_i x_i) (\\sum_j w_j y_j).\n",
    "    num1 = reducer(x * y) * 2.0\n",
    "    den1 = reducer(math_ops.square(x) + math_ops.square(y))\n",
    "    c2 *= compensation\n",
    "    cs = (num1 - num0 + c2) / (den1 - den0 + c2)\n",
    "\n",
    "    # SSIM score is the product of the luminance and contrast-structure measures.\n",
    "    return luminance, cs\n",
    "\n",
    "\n",
    "def _fspecial_gauss(size, sigma):\n",
    "    \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function.\"\"\"\n",
    "    size = ops.convert_to_tensor(size, dtypes.int32)\n",
    "    sigma = ops.convert_to_tensor(sigma)\n",
    "\n",
    "    coords = math_ops.cast(math_ops.range(size), sigma.dtype)\n",
    "    coords -= math_ops.cast(size - 1, sigma.dtype) / 2.0\n",
    "\n",
    "    g = math_ops.square(coords)\n",
    "    g *= -0.5 / math_ops.square(sigma)\n",
    "\n",
    "    g = array_ops.reshape(g, shape=[1, -1]) + \\\n",
    "        array_ops.reshape(g, shape=[-1, 1])\n",
    "    g = array_ops.reshape(g, shape=[1, -1])  # For tf.nn.softmax().\n",
    "    g = nn_ops.softmax(g)\n",
    "    return array_ops.reshape(g, shape=[size, size, 1, 1])\n",
    "\n",
    "\n",
    "def _ssim_per_channel(img1, img2, max_val=1.0):\n",
    "    \"\"\"Computes SSIM index between img1 and img2 per color channel.\n",
    "    This function matches the standard SSIM implementation from:\n",
    "    Wang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image\n",
    "    quality assessment: from error visibility to structural similarity. IEEE\n",
    "    transactions on image processing.\n",
    "    Details:\n",
    "      - 11x11 Gaussian filter of width 1.5 is used.\n",
    "      - k1 = 0.01, k2 = 0.03 as in the original paper.\n",
    "    Args:\n",
    "      img1: First image batch.\n",
    "      img2: Second image batch.\n",
    "      max_val: The dynamic range of the images (i.e., the difference between the\n",
    "        maximum the and minimum allowed values).\n",
    "    Returns:\n",
    "      A pair of tensors containing and channel-wise SSIM and contrast-structure\n",
    "      values. The shape is [..., channels].\n",
    "    \"\"\"\n",
    "    filter_size = constant_op.constant(11, dtype=dtypes.int32)\n",
    "    filter_sigma = constant_op.constant(1.5, dtype=img1.dtype)\n",
    "\n",
    "    shape1, shape2 = array_ops.shape_n([img1, img2])\n",
    "    checks = [\n",
    "        control_flow_ops.Assert(math_ops.reduce_all(math_ops.greater_equal(\n",
    "            shape1[-3:-1], filter_size)), [shape1, filter_size], summarize=8),\n",
    "        control_flow_ops.Assert(math_ops.reduce_all(math_ops.greater_equal(\n",
    "            shape2[-3:-1], filter_size)), [shape2, filter_size], summarize=8)]\n",
    "\n",
    "    # Enforce the check to run before computation.\n",
    "    with ops.control_dependencies(checks):\n",
    "        img1 = array_ops.identity(img1)\n",
    "\n",
    "    # TODO(sjhwang): Try to cache kernels and compensation factor.\n",
    "    kernel = _fspecial_gauss(filter_size, filter_sigma)\n",
    "    kernel = array_ops.tile(kernel, multiples=[1, 1, shape1[-1], 1])\n",
    "\n",
    "    # The correct compensation factor is `1.0 - tf.reduce_sum(tf.square(kernel))`,\n",
    "    # but to match MATLAB implementation of MS-SSIM, we use 1.0 instead.\n",
    "    compensation = 1.0\n",
    "\n",
    "    # TODO(sjhwang): Try FFT.\n",
    "    # TODO(sjhwang): Gaussian kernel is separable in space. Consider applying\n",
    "    #   1-by-n and n-by-1 Gaussain filters instead of an n-by-n filter.\n",
    "    def reducer(x):\n",
    "        shape = array_ops.shape(x)\n",
    "        x = array_ops.reshape(x, shape=array_ops.concat([[-1], shape[-3:]], 0))\n",
    "        y = nn.depthwise_conv2d(\n",
    "            x, kernel, strides=[1, 1, 1, 1], padding='VALID')\n",
    "        return array_ops.reshape(y, array_ops.concat([shape[:-3],\n",
    "                                                      array_ops.shape(y)[1:]], 0))\n",
    "\n",
    "    luminance, cs = _ssim_helper(img1, img2, reducer, max_val, compensation)\n",
    "\n",
    "    # Average over the second and the third from the last: height, width.\n",
    "    axes = constant_op.constant([-3, -2], dtype=dtypes.int32)\n",
    "    ssim_val = math_ops.reduce_mean(luminance * cs, axes)\n",
    "    cs = math_ops.reduce_mean(cs, axes)\n",
    "    return ssim_val, cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Try structural similarity index metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from backport_ssim import _ssim_per_channel\n",
    "g_roi_ssim = tf.Graph()\n",
    "with g_roi_ssim.as_default():\n",
    "    init = tf.global_variables_initializer()\n",
    "    # tf Graph Input\n",
    "    fixed_img = tf.placeholder(\"float\", shape=(\n",
    "        1, None, None, 1), name='FixedImage')\n",
    "    moving_img = tf.placeholder(\"float\", shape=(\n",
    "        1, None, None, 1), name='MovingImage')\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "\n",
    "    with tf.name_scope('transform_parameters'):  # Set transform parameters\n",
    "        x_offset = tf.Variable(0.0, name=\"x_offset\")\n",
    "        y_offset = tf.Variable(0.0, name=\"y_offset\")\n",
    "        # we keep rotation fixed\n",
    "        rotation = tf.placeholder(\"float\", shape=tuple(), name=\"rotation\")\n",
    "\n",
    "    with tf.name_scope('transformer_and_interpolator'):\n",
    "        flat_mat = tf.tile([tf.cos(rotation), -tf.sin(rotation), x_offset,\n",
    "                            tf.sin(rotation), tf.cos(rotation), y_offset], (1,))\n",
    "        flat_mat = tf.reshape(flat_mat, (1, 6))\n",
    "        trans_tensor = affine_transform(moving_img, flat_mat)\n",
    "\n",
    "    with tf.name_scope('metric'):\n",
    "        ssim, _ = _ssim_per_channel(fixed_img[:, 20:75, 25:100, :]/255.0,\n",
    "                                    trans_tensor[:, 20:75, 25:100, :]/255.0,\n",
    "                                    max_val=1.0)\n",
    "        mssim = tf.reduce_mean(ssim, name='MeanSSIM')\n",
    "        rev_mssim = 1-mssim  # since we can only minimize\n",
    "        optimizer = tf.train.GradientDescentOptimizer(5e-2).minimize(rev_mssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "tab",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from matplotlib import patches\n",
    "optimize_iters = 40\n",
    "loss_history = []\n",
    "with tf.Session(graph=g_roi_ssim) as sess:\n",
    "    plt.close('all')\n",
    "    fig, m_axs = plt.subplots(2, 3, figsize=(11, 5), dpi=100)\n",
    "    tf.initialize_all_variables().run()\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    # Fit all training data\n",
    "    const_feed_dict = make_feed_dict(bw_img, shift_img)\n",
    "\n",
    "    def update_frame(i):\n",
    "        global loss_history\n",
    "        (ax1, ax2, ax5), (ax3, ax4, ax6) = m_axs\n",
    "        for c_ax in m_axs.flatten():\n",
    "            c_ax.cla()\n",
    "            c_ax.axis('off')\n",
    "        f_ssim, x_pos, y_pos, rs_img = sess.run([mssim, x_offset, y_offset, trans_tensor],\n",
    "                                                feed_dict=const_feed_dict)\n",
    "        loss_history += [f_ssim]\n",
    "\n",
    "        ax1.imshow(bw_img, cmap='bone')\n",
    "        ax1.set_title('$T_0$')\n",
    "        ax2.imshow(shift_img, cmap='bone')\n",
    "        ax2.set_title('$T_1$')\n",
    "        ax3.imshow(rs_img[0, :, :, 0], cmap='bone')\n",
    "        ax3.set_title('Output')\n",
    "        ax4.imshow(bw_img*1.0-rs_img[0, :, :, 0],\n",
    "                   cmap='RdBu', vmin=-100, vmax=100)\n",
    "        ax4.set_title('Difference\\nSSIM: %2.2f' % (f_ssim))\n",
    "        rect = patches.Rectangle(\n",
    "            (25, 20), 75, 55, linewidth=2, edgecolor='g', facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax4.add_patch(rect)\n",
    "        ax5.plot(loss_history)\n",
    "        ax5.set_xlabel('Iteration')\n",
    "        ax5.set_ylabel('SSIM')\n",
    "        ax5.axis('on')\n",
    "\n",
    "        for _ in range(1):\n",
    "            sess.run(optimizer, feed_dict=const_feed_dict)\n",
    "    # write animation frames\n",
    "    anim_code = FuncAnimation(fig,\n",
    "                              update_frame,\n",
    "                              frames=optimize_iters,\n",
    "                              interval=1000,\n",
    "                              repeat_delay=2000).to_html5_video()\n",
    "    plt.close('all')\n",
    "HTML(anim_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ITK and Simple ITK\n",
    "For medical imaging the standard tools used are ITK and SimpleITK and they have been optimized over decades to deliver high-performance registration tasks. They are a bit clumsy to use from python, but they offer by far the best established tools for these problems.\n",
    "\n",
    "[https://itk.org/ITKSoftwareGuide/html/Book2/ITKSoftwareGuide-Book2ch3.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "\n",
    "def register_img(fixed_arr,\n",
    "                 moving_arr,\n",
    "                 use_affine=True,\n",
    "                 use_mse=True,\n",
    "                 brute_force=True):\n",
    "    fixed_image = sitk.GetImageFromArray(fixed_arr)\n",
    "    moving_image = sitk.GetImageFromArray(moving_arr)\n",
    "    transform = sitk.AffineTransform(\n",
    "        2) if use_affine else sitk.ScaleTransform(2)\n",
    "    initial_transform = sitk.CenteredTransformInitializer(sitk.Cast(fixed_image, moving_image.GetPixelID()),\n",
    "                                                          moving_image,\n",
    "                                                          transform,\n",
    "                                                          sitk.CenteredTransformInitializerFilter.GEOMETRY)\n",
    "    ff_img = sitk.Cast(fixed_image, sitk.sitkFloat32)\n",
    "    mv_img = sitk.Cast(moving_image, sitk.sitkFloat32)\n",
    "    registration_method = sitk.ImageRegistrationMethod()\n",
    "    if use_mse:\n",
    "        registration_method.SetMetricAsMeanSquares()\n",
    "    else:\n",
    "        registration_method.SetMetricAsMattesMutualInformation(\n",
    "            numberOfHistogramBins=50)\n",
    "\n",
    "    if brute_force:\n",
    "        sample_per_axis = 12\n",
    "        registration_method.SetOptimizerAsExhaustive(\n",
    "            [sample_per_axis//2, 0, 0])\n",
    "        # Utilize the scale to set the step size for each dimension\n",
    "        registration_method.SetOptimizerScales(\n",
    "            [2.0*3.14/sample_per_axis, 1.0, 1.0])\n",
    "    else:\n",
    "        registration_method.SetMetricSamplingStrategy(\n",
    "            registration_method.RANDOM)\n",
    "        registration_method.SetMetricSamplingPercentage(0.25)\n",
    "\n",
    "    registration_method.SetInterpolator(sitk.sitkLinear)\n",
    "\n",
    "    registration_method.SetOptimizerAsGradientDescent(learningRate=1.0,\n",
    "                                                      numberOfIterations=200,\n",
    "                                                      convergenceMinimumValue=1e-6,\n",
    "                                                      convergenceWindowSize=10)\n",
    "    # Scale the step size differently for each parameter, this is critical!!!\n",
    "    registration_method.SetOptimizerScalesFromPhysicalShift()\n",
    "\n",
    "    registration_method.SetInitialTransform(initial_transform, inPlace=False)\n",
    "    final_transform_v1 = registration_method.Execute(ff_img,\n",
    "                                                     mv_img)\n",
    "    print('Optimizer\\'s stopping condition, {0}'.format(\n",
    "        registration_method.GetOptimizerStopConditionDescription()))\n",
    "    print('Final metric value: {0}'.format(\n",
    "        registration_method.GetMetricValue()))\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetReferenceImage(fixed_image)\n",
    "\n",
    "    # SimpleITK supports several interpolation options, we go with the simplest that gives reasonable results.\n",
    "    resample.SetInterpolator(sitk.sitkBSpline)\n",
    "    resample.SetTransform(final_transform_v1)\n",
    "    return sitk.GetArrayFromImage(resample.Execute(moving_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "reg_img = register_img(bw_img, shift_img, brute_force=False, use_mse=True)\n",
    "print(reg_img.max(), bw_img.max())\n",
    "fig, (ax1, ax2, ax2d, ax3, ax4) = plt.subplots(1, 5, figsize=(20, 5), dpi=100)\n",
    "ax1.imshow(bw_img, cmap='bone')\n",
    "ax1.set_title('$T_0$')\n",
    "ax2.imshow(shift_img, cmap='bone')\n",
    "ax2.set_title('$T_1$')\n",
    "ax2d.imshow(1.0*bw_img-shift_img, cmap='RdBu', vmin=-100, vmax=100)\n",
    "ax2d.set_title('$T_1$ Registerd Difference')\n",
    "ax3.imshow(reg_img, cmap='bone')\n",
    "ax3.set_title('$T_1$ Registered')\n",
    "ax4.imshow(1.0*bw_img-reg_img, cmap='RdBu', vmin=-127, vmax=127)\n",
    "ax3.set_title('$T_1$ Registerd Difference');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Subdividing the data\n",
    "\n",
    "We can approach the problem by subdividing the data into smaller blocks and then apply the digital volume correlation independently to each block.\n",
    "- information on changes in different regions\n",
    "- less statistics than a larger box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducing Physics\n",
    "\n",
    "\n",
    "DIC or DVC by themselves include no _sanity check_ for realistic offsets in the correlation itself. The method can, however be integrated with physical models to find a more optimal solutions.\n",
    "\n",
    "- information from surrounding points\n",
    "- smoothness criteria\n",
    "- maximum deformation / force\n",
    "- material properties\n",
    "\n",
    "\n",
    "\n",
    "$$ C_{\\textrm{cost}} = \\underbrace{C_{I_0,I_1}(\\vec{r})}_{\\textrm{Correlation Term}} + \\underbrace{\\lambda ||\\vec{r}||}_{\\textrm{deformation term}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Distribution Metrics\n",
    "\n",
    "\n",
    "As we covered before distribution metrics like the distribution tensor can be used for tracking changes inside a sample. Of these the most relevant is the texture tensor from cellular materials and liquid foam. The texture tensor is the same as the distribution tensor except that the edges (or faces) represent physically connected / touching objects rather than touching Voronoi faces (or conversely Delaunay triangles).\n",
    "\n",
    "These metrics can also be used for tracking the behavior of a system without tracking the single points since most deformations of a system also deform the distribution tensor and can thus be extracted by comparing the distribution tensor at different time steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quantifying Deformation: Strain \n",
    "\n",
    "\n",
    "We can take any of these approaches and quantify the deformation using a tool called the strain tensor. \n",
    "\n",
    "Strain is defined in mechanics for the simple 1D case as the change in the length against the change in the original length.\n",
    "\n",
    "$$ e = \\frac{\\Delta L}{L} $$ \n",
    "\n",
    "While this defines the 1D case well, it is difficult to apply such metrics to voxel, shape, and tensor data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Strain Tensor\n",
    "\n",
    "There are a number of different ways to calculate strain and the strain tensor, but the most applicable for general image based applications is called the [infinitesimal strain tensor](http://en.wikipedia.org/wiki/Infinitesimal_strain_theory), because the element matches well to square pixels and cubic voxels.\n",
    "\n",
    "![Strain Model](https://upload.wikimedia.org/wikipedia/commons/2/23/2D_geometric_strain.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Types of Strain\n",
    "\n",
    "We catagorize the types of strain into two main catagories:\n",
    "\n",
    "$$ \\underbrace{\\mathbf{E}}_{\\textrm{Total Strain}} = \\underbrace{\\varepsilon_M \\mathbf{I_3}}_{\\textrm{Volumetric}} + \\underbrace{\\mathbf{E}^\\prime}_{\\textrm{Deviatoric}} $$\n",
    "\n",
    "### Volumetric / Dilational\n",
    "\n",
    "The isotropic change in size or scale of the object. \n",
    "\n",
    "### Deviatoric\n",
    "\n",
    "The change in the proportions of the object (similar to anisotropy) independent of the final scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Two Point Correlation - Volcanic Rock\n",
    "\n",
    "\n",
    "__Data provided by Mattia Pistone and Julie Fife__\n",
    "\n",
    "The air phase changes from small very anisotropic bubbles to one large connected pore network. \n",
    "- The same tools cannot be used to quantify those systems. \n",
    "- Furthermore there are motion artifacts which are difficult to correct.\n",
    "\n",
    "<video controls src=\"ext-figures/roicurv.m4v\" type=\"video/m4v\">\n",
    "Your browser does not support the video tag.\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can utilize the two point correlation function of the material to characterize the shape generically for each time step and then compare.\n",
    "\n",
    "<video controls src=\"ext-figures/rdfanim.m4v\" type=\"video/m4v\">\n",
    "Your browser does not support the video tag.\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Dynamic experiments\n",
    "- Object tracking\n",
    "- Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "footer": "April 9, 2020 - ETH 227-0966-00L: Quantitative Big Imaging/Dynamic experiments",
   "header": "<table width='100%' style='margin: 0px;'><tr><td align='left'><img src='../common/figures/eth_logo_kurz_pos.svg' style='height:30px;'></td><td align='right'><img src='../common/figures/PSI-Logo.svg' style='height:50px;'></td></tr></table>"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "scroll": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "387.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
